[{"authors":["admin"],"categories":null,"content":"I am a Machine Learning Engineer with 4 years of experience working in the field of machine learning, in particular computer vision problems.\nCurrently I am an ML Engineer at CubicFarms Corp.in Vancouver where I am solving some really exciting problems at the intersection of indoor farming+ ML at scale.\nI graduated from Simon Fraser University, Canada where I was a Research Assistant at Networked Robotics and Sensing Laboratory and worked under an amazing advisor Prof. Shahram Payandeh. I worked on problem of semi-supervised detection-based-tracking using kinect based RGB and depth images for a mobile robot. I also spent some great time at MILA, Quebec where I worked on with Dr. David Rolnick on biodiversity-monitoring project under Microsoft AI for Earth Grant.\nIn past, I was\n a Machine Learning Scientist at Coastal Resource Mapping, open-source software developer for Ivadomed. a team I highly recommend to work with if you are serious about research with software development and worked at UrtheCast on large scale satellite data  In (long) past, I was a part of Helicopter and VTOL Laboratory, IIT Kanpur (supervised by Prof. Abhishek), Samsung IoT Innovation Lab and Applied Cognitive Science Lab, IIT Mandi.\nOver the course of my work experience, I have developed excellent skills in PyTorch, Python, MATLAB, Numpy, Scipy, OpenCV, Matplotlib, Docker, etc. scientific stack as well as cloud services like AWS and Azure.\nI am an active member of developer community groups of Vancouver where I run Women in Machine Learning and Data Science meetup. As a strong proponent of tech and diversity, my involvement goes beyond local community work. I am a co-organizer of the Computer Vision Interest Group at Machine Learning Collective which brings people from the industry, academia, and enthusiasts to discuss papers and hack on projects together. In past, I have been one of the the chairs of Women in Computer Vision workshop co-hosted with CVPR, 2020 and was on the committe of the Women in Machine Learning workshop, 2019. I was also an advisor for the Women in Computer Vision workshop, 2021 which was co-hosted with CVPR, 2021.\nIf you have questions about any of my work, feel free to reach out to me via email.\nNote: If you are someone who wants to chat about Masters (especially research-track) in Canada, DO-NOT hesitate to email me. It\u0026rsquo;s a funded program and I\u0026rsquo;ll be more than happy to tell you about it!\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://srishti.dev/author/srishti-yadav/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/srishti-yadav/","section":"authors","summary":"I am a Machine Learning Engineer with 4 years of experience working in the field of machine learning, in particular computer vision problems.\nCurrently I am an ML Engineer at CubicFarms Corp.","tags":null,"title":"Srishti Yadav","type":"authors"},{"authors":["Maryamsadat Rasoulidanesh","Srishti Yadav","Sachini Herath","Yasaman Vaghei","Shahram Payandeh"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   Supplementary notes can be added here, including [code and math](https://sourcethemes.com/academic/docs/writing-markdown-latex/). -- ","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"69f20fce694a3d92aebf009185695fcd","permalink":"https://srishti.dev/publication/deep-attention-models-for-human-tracking-using-rgbd/","publishdate":"2019-01-01T00:00:00Z","relpermalink":"/publication/deep-attention-models-for-human-tracking-using-rgbd/","section":"publication","summary":"Visual tracking performance has long been limited by the lack of better appearance models. These models fail either where they tend to change rapidly, like in motion-based tracking, or where accurate information of the object may not be available, like in color camouflage (where background and foreground colors are similar). This paper proposes a robust, adaptive appearance model which works accurately in situations of color camouflage, even in the presence of complex natural objects. The proposed model includes depth as an additional feature in a hierarchical modular neural framework for online object tracking. The model adapts to the confusing appearance by identifying the stable property of depth between the target and the surrounding object(s). The depth complements the existing RGB features in scenarios when RGB features fail to adapt, hence becoming unstable over a long duration of time. The parameters of the model are learned efficiently in the Deep network, which consists of three modules:(1) The spatial attention layer, which discards the majority of the background by selecting a region containing the object of interest; (2) the appearance attention layer, which extracts appearance and spatial information about the tracked object; and (3) the state estimation layer, which enables the framework to predict future object appearance and location. Three different models were trained and tested to analyze the effect of depth along with RGB information. Also, a model is proposed to utilize only depth as a standalone input for tracking purposes. The proposed models were also evaluated in real-time using KinectV2 and showed very promising results. The results of our proposed network structures and their comparison with the state-of-the-art RGB tracking model demonstrate that adding depth significantly improves the accuracy of tracking in a more challenging environment (i.e., cluttered and camouflaged environments). Furthermore, the results of depth-based models showed that depth data can provide enough information for accurate tracking, even without RGB information.","tags":["Source Themes"],"title":" Deep Attention Models for Human Tracking Using RGBD (Poster @NeurIPS, 2020)","type":"publication"},{"authors":["Srishti Yadav","Shahram Payandeh"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   Supplementary notes can be added here, including [code and math](https://sourcethemes.com/academic/docs/writing-markdown-latex/). -- ","date":1541030400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1541030400,"objectID":"dc8da6b4fe376e635f00805ad77a29f7","permalink":"https://srishti.dev/publication/kcfexperimentalstudy/","publishdate":"2018-11-01T00:00:00Z","relpermalink":"/publication/kcfexperimentalstudy/","section":"publication","summary":"Over years correlation filter-based trackers have proved their worth with their increased efficiency and increased computation speed. Kernelized Correlation Filter (KCF) was one such attempt which, by using kernel trick, achieved compelling result as compared to traditional correlation filter-based trackers. In this paper, our goal is to analyze this tracker to observe its strengths and weaknesses in detail. We use Kinect RGB camera for our experimental analysis and report our findings. The analysis showed that KCF is not only computationally very fast, it is time-invariant and very robust to speed and vertical motions. However, it is not very robust to illumination variations, scale and color.","tags":["Source Themes"],"title":"Real-Time Experimental Study of Kernelized Correlation Filter Tracker using RGB Kinect Camera","type":"publication"},{"authors":["Pratik Chaturvedi","Kamal Kishore Thakur","Naresh Mali","Venkata Uday Kala","Sudhakar Kumar","Srishti Yadav","Varun Dutt"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   Supplementary notes can be added here, including [code and math](https://sourcethemes.com/academic/docs/writing-markdown-latex/). -- ","date":1525132800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1525132800,"objectID":"68c2049ad763407b982f7539fa0a174e","permalink":"https://srishti.dev/publication/a-low-cost-iot-framework-for-landslide-prediction-and-risk-communication/","publishdate":"2018-05-01T00:00:00Z","relpermalink":"/publication/a-low-cost-iot-framework-for-landslide-prediction-and-risk-communication/","section":"publication","summary":"In this book chapter, we propose the design and development of a low-cost IoT framework formonitoring landslide is discussed. This framework involved the use of MEMS-based sensors for monitoring landslides at the lab scale. The proposed frame-work can monitor soil moisture and movement and generate alerts based onpredefined thresholds.","tags":["Source Themes"],"title":"A Low-Cost IoT Framework for Landslide Prediction and Risk Communication","type":"publication"},{"authors":["Mali Naresh","Pratik Chaturvedi","Srishti Yadav","Varun Dutt","Kala Venkat Uday"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   Supplementary notes can be added here, including [code and math](https://sourcethemes.com/academic/docs/writing-markdown-latex/). -- ","date":1498867200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1498867200,"objectID":"2b0df95e8b670a5265685b036c94aacc","permalink":"https://srishti.dev/publication/training-of-sensors-for-early-warning-system-of-rainfall-induced-landslides/","publishdate":"2017-12-01T00:00:00Z","relpermalink":"/publication/training-of-sensors-for-early-warning-system-of-rainfall-induced-landslides/","section":"publication","summary":"Changes in the Earth’s climate are likely to increase natural hazards such as drought, floods, earthquakes,landslides, etc. The present study focusing on to early warning systems (EWS) of landslides, major issues in Himalayan regionwithout prominence to deforestation, encroachments and un-engineered cutting of slopes and reforming for infrastructuralpurposes. EWS can be depicted by conducting a series of flume tests using micro-electro mechanical systems sensors data afterreaching threshold values under controlled laboratory conditions. Based on the threshold value database, an alert will be sentvia SMS","tags":["Source Themes"],"title":"Training of Sensors for Early Warning System of Rainfall Induced Landslides","type":"publication"},{"authors":null,"categories":null,"content":"When working with python libraries, especially for visualization, I usually get confused my number of options available for plotting. Example:\n1. plt.plot() 2. ax = plt.subplot() ax.plot(x, y) 3. fig1, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2) 4. f, axarr = plt.subplots(2,2) axarr[0,0].imshow(image_datas[0]) ....  After reading through a bunch of stackoverflow explainations, I compiled them here:\n Question 1: What is the difference between drawing plots using plot, axes or figure in matplotlib?  Plot just one figure with (x,y) coordinates\nplt.plot(x, y)  If you just want to get one graphic, you can use this way.\nExample:\nimport numpy as np import matplotlib.pyplot as plt x = np.random.rand(10) y = np.random.rand(10) figure1 = plt.plot(x,y)    Fig 1. Photo via Stackoverflow   Plot one or several figure(s) in the same window\nPlot just one figure\nax = plt.subplot() ax.plot(x, y)  or you can plot multiple figures like this:\nfig1, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2)  This plot 4 figures which are named ax1, ax2, ax3 and ax4 each one but on the same window. This window will be just divided in 4 parts with my example.\nExample:\nimport numpy as np import matplotlib.pyplot as plt x1 = np.random.rand(10) x2 = np.random.rand(10) x3 = np.random.rand(10) x4 = np.random.rand(10) y1 = np.random.rand(10) y2 = np.random.rand(10) y3 = np.random.rand(10) y4 = np.random.rand(10) figure2, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2) ax1.plot(x1,y1) ax2.plot(x2,y2) ax3.plot(x3,y3) ax4.plot(x4,y4) plt.show()    Fig 2. Photo via Stackoverflow   Another method for multiple plots\n import numpy as np import matplotlib.pyplot as plt fig = plt.figure() new_plot = fig.add_subplot(111) new_plot.plot(x, y)    Question 2: Difference between “axes” and “axis” in matplotlib? In the context of matplotlib, axes is not the plural form of axis, it actually denotes the plotting area, including all axis.\n  Fig 3. Photo via matplotlib old documentation     Axes\nThis is what you think of as a plot, it is the region of the image with the data space (marked as the inner blue box). A given figure can contain many Axes, but a given Axes object can only be in one Figure. The Axes contains two (or three in the case of 3D) Axis objects (be aware of the difference between Axes and Axis) which take care of the data limits (the data limits can also be controlled via set via the set_xlim() and set_ylim() Axes methods). Each Axes has a title (set via set_title()), an x-label (set via set_xlabel()), and a y-label set via set_ylabel()).\n  Axis\nThese are the number-line-like objects (circled in green). They take care of setting the graph limits and generating the ticks (the marks on the axis) and ticklabels (strings labeling the ticks). The location of the ticks is determined by a Locator object and the ticklabel strings are formatted by a Formatter. The combination of the correct Locator and Formatter gives very fine control over the tick locations and labels.\n    Question 3: What is the difference between plt.subplots() and plt.figure() In matplotlib, we can plots in two ways like below:\nplt.figure(1,figsize=(400,8))  or\nfig,ax = plt.subplots() fig.set_size_inches(400,8)  and though both are correct, they have their differences.\nplt.figure just creates a figure (but with no axes in it) whereas plt.subplots takes optional arguments (ex: plt.subplots(2, 2)) to create an array of axes in the figure. Most of the kwargs that plt.figure takes plt.subplots also takes.\nplt.figure() is usually used when you want more customization to you axes, such as positions, sizes, colors and etc. You can see artist tutorial for more details. (I personally prefer this for individual plot).\nplt.subplots() is recommended for generating multiple subplots in grids. You can also achieve higher flexibility using \u0026lsquo;gridspec\u0026rsquo; and \u0026lsquo;subplots\u0026rsquo;, see details here.\n","date":-27077689320,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-27077689320,"objectID":"5331e734fd177fca40d4f6a0b3381ab4","permalink":"https://srishti.dev/post/1111-11-11-plt-fig-axis/","publishdate":"1111-12-11T12:38:00-07:00","relpermalink":"/post/1111-11-11-plt-fig-axis/","section":"post","summary":"When working with python libraries, especially for visualization, I usually get confused my number of options available for plotting. Example:\n1. plt.plot() 2. ax = plt.subplot() ax.plot(x, y) 3. fig1, ((ax1, ax2), (ax3, ax4)) = plt.","tags":"technical","title":"Understanding Usage of plt, figure, subplot, axes, axis in matplotlib","type":"post"},{"authors":null,"categories":null,"content":"Paper: Learning Transferable Visual Models From Natural Language Supervision\n  PDF: Learning Transferable Visual Models From Natural Language Supervision\n  Blog: CLIP: Connecting Text and Images\n  General Terms: \n  Contrastive Learning: It is based on the intuition that you can contrast/differentiate between similar and dissimilar things. In machine learning model, we formulate this as a task of finding similar and dissimilar things i.e. the model should be able to classify between similar and dissimilar images\n  Zero-shot Learning : Zero-shot learning is a problem, where at test stage,the model/learner aims to recognize objects whose instances maynot have been seen during training. To learn about various works that have been done in this space (atleast till 2018), this paper provides good details: Zero-shot learning—a comprehensive evaluation of the good, the bad and the ugly.\n   The paper describes an approach where it takes a large dataset of image text pair and tries to learn a model that scores whether a image and text could co-occur. This is learned over a large dataset.\nQuestion: How to do classification zero shot i.e. without any training?\nGiven a specified classification task where there are some images and some labels and you are supposed to evaluate based on our prediction, you will embed all text labels into vectors and images into vectors and them compare the score of their cross product. The pairing that is going to give the highest score is going to be the prediction label.\n  Fig 1. Photo via Open AI Paper   In Fig.1, $N$ is the size of images associated with some text. $T_i$ is the encoding for the entire text string.\nWhy is it zero shot? This model is zero shot because it refers to the number of lables we see for training. In this architecture, for a particular evaluation task, you do not train. You take the train weights(and give some bias). THis notion of zero shot is in line with GPT notion of zero shot i.e. you do a lot of training - you don\u0026rsquo;t care what is happenng at the pre-training stage\nDataset:\n Called WiT Dataset (WebImageText) 400 million (image, text) pairs by searching over text queries 500,000 text queries:  All words occuring atleast 100 times in the English version of Wikipedia + WordNet synsets + some details   Cap at a maximum of 20000 (image, text) pairs per query Total wordcount similar to WebText dataset used by GPT-2  Note: There is a difference between queries and labels. Example, when you give a query \u0026lsquo;dog\u0026rsquo; on google image, it will come with all sorts of images of \u0026lsquo;dog\u0026rsquo; and each such image will come associated with a (paired)text. These texts can be in the form of alt text or title of the page.\n  Fig 2. Photo via Open AI Paper   Fig.2 shows various methods authors use to test which pre-training method is better. We know there are various training method that connects image and language. Most vanilla verison is captioning (language modelling with images). In this paper, authors try a variety of pre-training methods. First for captioning - they found it is not very compute efficient. Second, they tried BoW (bag of words) and they find that they scale better. However, the best they found was contrastive learning.\nModel architecture:\n Image Encode \n ResNet  Modifications: anti-aliased max pooling; final global pooling by QK Value (QKV) attention Scaling model size by allocating compute equally (to width, depth, input resolution) OR   Vision Transformer    Text Encoder \n Transformer text encoder  scaling model size by width only; do not scale depth      Other parameters   $N$ x $N$ affinity matrix. Symmetric Contrastic Loss Temperature $t$ is initialized to 0.07 but allowed to learn Adam with decoupled weight decay ($adam_w$). Cosine LR. Batch size 32768 (this is huge!)     * { box-sizing: border-box; } .column img { width: 100%; float: left ; margin: 0; } .column { width: 50%; float: left ; padding: 10px; } .row::after { content: \"\"; clear: both; display: table; }        Fig 3a. Photo via Open AI Paper    Fig 3b. Photo via Open AI Paper        Prompt Engineering:\nFrom Fig 3.a, we can see that in addition to texts, authors have also experimented using prompts wth texts i.e. you can embed the label itself but you can also embed string with a customized prompt ahead of it. E.g. \u0026lsquo;a photo of \u0026lsquo;, \u0026lsquo;a bag of\u0026rsquo;. Fig 3.b shows that authors show that prompts work better than raw label (+15%)\nPerformance:\n  Fig 4. Photo via Open AI Paper from Appendix Section   For ImageNet (1.2 million), vanilla ResNet50 gives accuracy of 56.3% [1] and zero-shot (CLIP model) gives 59.6%.(see last column of first row in Fig.4). Model (L/14-336x) that gives classification accuracy close to ResNet-50 uses a lot more compute (see last column, last row in Fig.4). L/14-336x means 14 large with input resolution 336px. Hence, to surpass a supervised ResNet, you need a lot bigger model\n [1] : https://arxiv.org/pdf/1811.06992.pdf\nAcknowledment: Thanks to the discussion in TTIC reading group which introduced me to this paper.\n","date":-27080281320,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-27080281320,"objectID":"138b5b4f7d9acb14268b4c171582495a","permalink":"https://srishti.dev/post/1111-11-11-clip-connecting-text-and-images/","publishdate":"1111-11-11T12:38:00-07:00","relpermalink":"/post/1111-11-11-clip-connecting-text-and-images/","section":"post","summary":"Paper: Learning Transferable Visual Models From Natural Language Supervision\n  PDF: Learning Transferable Visual Models From Natural Language Supervision\n  Blog: CLIP: Connecting Text and Images\n  General Terms:","tags":"technical","title":"CLIP: Connecting Text and Images","type":"post"},{"authors":null,"categories":null,"content":" iNat2017   Data: https://github.com/visipedia/inat_comp/tree/master/2017  iNat2018 and iNat2019   Data: https://github.com/visipedia/inat_comp/blob/master/2018/README.md  Data: https://github.com/visipedia/inat_comp  Details: The dataset is similar to iNat2017 with small differences, which are mentioned in the website.  Herbarium Dataset   Paper: https://drive.google.com/file/d/1HPyY82IwGkKlp3ow13JCDtn1G-s_mgGJ/view  Cassava (leaves) images   Paper: https://drive.google.com/file/d/1GW0Ak_fS0ZMXcy89B7di1xNF1MBIga_4/view  Birds   Paper: https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Horn_Building_a_Bird_2015_CVPR_paper.pdf  Animal Species (camera trap)   Paper: https://arxiv.org/pdf/2004.10340.pdf  UCSD Birds 200   Data: http://www.vision.caltech.edu/visipedia/CUB-200.html Paper: https://authors.library.caltech.edu/27452/1/CUB_200_2011.pdf  Birdsnap Large-scale Fine-grained Visual Categorization of Birds   Data: Link doesn\u0026rsquo;t work anymore. Paper: https://openaccess.thecvf.com/content_cvpr_2014/papers/Berg_Birdsnap_Large-scale_Fine-grained_2014_CVPR_paper.pdf  Stanford Dogs   Data: http://vision.stanford.edu/aditya86/ImageNetDogs/ Paper: https://people.csail.mit.edu/khosla/papers/fgvc2011.pdf  Oxford Dogs   Data:https://www.robots.ox.ac.uk/~vgg/data/pets/ Paper:https://www.robots.ox.ac.uk/~vgg/publications/2012/parkhi12a/parkhi12a.pdf  Flowers   Data:Link mentioned in the paper but doesn\u0026rsquo;t work. Couldn\u0026rsquo;t find the datasest Paper:https://www.ics.uci.edu/~welling/teaching/273ASpring09/nilsback06.pdf  ","date":-27080281320,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-27080281320,"objectID":"7f5e156454ffd891a88c5e967d764fea","permalink":"https://srishti.dev/post/1111-11-11-datasets-for-fine-grained-image-classification/","publishdate":"1111-11-11T12:38:00-07:00","relpermalink":"/post/1111-11-11-datasets-for-fine-grained-image-classification/","section":"post","summary":"iNat2017   Data: https://github.com/visipedia/inat_comp/tree/master/2017  iNat2018 and iNat2019   Data: https://github.com/visipedia/inat_comp/blob/master/2018/README.md  Data: https://github.com/visipedia/inat_comp  Details: The dataset is similar to iNat2017 with small differences, which are mentioned in the website.","tags":"technical","title":"Datasets for Fine-Grained Image Classification","type":"post"},{"authors":null,"categories":null,"content":"In deep learning, it is not easy to tune hyperparameters for optimal results. If we have 2 parameters (each with 3 prior desirable values), it is an easier problem. We will have possible combinations to try. However, with more parameters, the possible combinations will increase exponentially. For example, for 5 parameters, each with 4 desired values, we will have possible combinations. Manually trying each of them is not a very practical approach.\nhence, the question usually is \u0026ldquo;Which way should I update my hyper-parameter to reduce the loss (i.e. gradients) in order to find the optimal model architecture?\nIdealy, we should come up with an approach :\n where we can search for hyperparameters (hyper-parameter search space) using a distributed compute on the cloud. intelligently optimize which of the possible combinations from the search space will give us the best results. Supports exisitng optimization techniques like Grid Search, Random Search, Bayesian Optmization etc.  Some of the exisiting tools for hyperparamter tuning are:\n RayTune (https://ray.readthedocs.io/en/latest/tune.html) Talos (https://autonomio.github.io/talos) NNI (https://nni.readthedocs.io/en/latest/index.html) Orion (https://orion.readthedocs.io/en/latest/user/pytorch.html#adapting-the-code-for-orion) Sherpa (https://parameter-sherpa.readthedocs.io/en/latest/parallel/parallel-guide.html) TPOT (https://github.com/EpistasisLab/tpot) Hyperopt (https://github.com/hyperopt/hyperopt) AWS Sagemaker Hyperparameter tuning (https://aws.amazon.com/sagemaker/?hp=tile\u0026amp;so-exp=below) Botorch (https://botorch.org/)  Here, we will discuss hyperopt!\nHyperopt is an open-source hyperparameter tuning library written for Python. Hyperopt provides a general API for searching over hyperparameters and model types. Hyperopt offers two tuning algorithms: Random Search and the Bayesian method Tree of Parzen Estimators (TPE). To run hyperopt you define:\n the objective function the parameter space the number of experiments  There are both continuous and categorical methods to describe the parameters.\n-- Limitation: Hyper-parameter takes a lot of time to tune the parameters if number of trials and number of epocs (iteration of the neural network) are higher (which is desired). Hence, it would be good to explore how to parallelize the tuning work.\n","date":-27080281320,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-27080281320,"objectID":"6d8e0bb4c0c52ffa3b8e1e70191903af","permalink":"https://srishti.dev/post/1111-11-11-hyperopt/","publishdate":"1111-11-11T12:38:00-07:00","relpermalink":"/post/1111-11-11-hyperopt/","section":"post","summary":"In deep learning, it is not easy to tune hyperparameters for optimal results. If we have 2 parameters (each with 3 prior desirable values), it is an easier problem. We will have possible combinations to try.","tags":"technical","title":"Hyperopt: A tool for parameter tuning","type":"post"},{"authors":null,"categories":null,"content":"What is KL-Divergence KL Divergence is a measure of how one probabilty distributon is different from another. Some people also call it the distance between two distributions, however, strictly speaking it is not the distance. Distance is commutative while KL-Divergence is not.\nMathematically:\n$$ D_{KL}(P||Q) = \\sum_x P(X= x) log\\frac{P(X=x)}{Q(X=x)} $$\n$$ D_{KL}(P||Q) = \\sum_x P(X= x) log\\frac{P(X=x)}{Q(X=x)} $$\nwhere $\\sum_x P(X= x)$ is the summation of all the values that random variable $X$ will take and $P(X= x)$ is the probabilty of that random variable. In short we can also write:\n$$ D_{KL}(P||Q) = \\sum_x P(x) log\\frac{P(x)}{Q(x)} $$\n How can we generalize it to two different distributions?\nSuppose we have two multivariate normal distributions defined as\n$$ p(x) = N(x;p_1, \\Sigma_1 ) $$ $$ q(x) = N(x;p_2, \\Sigma_2 ) $$\nwhere $N$ is the normal distribution, $p_1$ \u0026amp; $p_2$ are are the means and $\\Sigma_1$ and $\\Sigma_2$ are the covariance matrix.\nThe multivariate normal distribution is defined as:\n$$ N(x;p;\\Sigma_1) = \\frac{1}{\\sqrt{(2\\pi)^{k} |\\Sigma_1|}} * exp(- \\frac{1}{2} (x-p)^{T} \\Sigma^{-1}(x-p)) $$\nif the two distributions have the same distributions. Here $x$ is the vector of length $k$ x $1$ i.e. the elements are $x= [x_1, x_2\u0026hellip;x_n]^{T}$\nNow, if we have two distributions as mutivariate normal density, then KL Divergence between the two normal distributions is defined as:\n$$ D_{KL}(p(x) || q(x)) = \\frac{1}{2} { \\bigg[log \\frac{|\\Sigma_{2}|}{|\\Sigma_{1}|} - d + tr(\\Sigma_2^{-1}\\Sigma_1) + (p_2 - p_1)^{T} \\Sigma_2^{-1}(p_2 - p_1)} \\bigg] $$\nProof:\nWe know that KL Divergence between two PDFs can be expressed as: $$ D_{KL}(p(x) || q(x)) = \\sum_x P(x) log\\frac{P(x)}{Q(x)} \\tag{1} $$\nand multi-variate normal distribution is defined like: $$ N(x;p;\\Sigma_1) = \\frac{1}{\\sqrt{(2\\pi)^{k} |\\Sigma_1|}} * exp\\Big[- \\frac{1}{2} (x-p_1)^{T} \\Sigma^{-1}(x-p_1)\\Big] \\tag{2} $$\nNote: $|\\Sigma|$ is the determinant and is not the absolute value\nTaking log of Eq.$2$, we can write as:\n$$ log P(x) = - \\frac{k}{2}log(2\\pi) - \\frac{1}{2}log(|\\Sigma_1|) - \\frac{1}{2} \\Big[(x-p_1)^{T} \\Sigma^{-1}(x-p_1)\\Big] \\tag{3} $$\nSimilarly for second probabilty distribution $Q$, $$ log Q(x) = - \\frac{k}{2}log(2\\pi) - \\frac{1}{2}log(|\\Sigma_2|) - \\frac{1}{2} \\Big[(x-p_2)^{T} \\Sigma^{-1}(x-p_2)\\Big] \\tag{4} $$\nNow, Eq.$\\tag{1}$ can be re-written as: $$ D_{KL}(p(x) || q(x)) = \\sum_x P(x) * [logP(x) - logQ(x)] \\tag{5} $$\nSubstituting Eq. $3$ and Eq. $4$ in Eq. $5$, we get:\n $$ \\begin{multline} D_{KL}(p(x) || q(x)) = \\sum_x P(x) * \\\\ \\hspace{25mm}- \\Big[\\frac{k}{2}log(2\\pi) - \\frac{1}{2}log(|\\Sigma_1|) - \\frac{1}{2} \\Big[(x-p_1)^{T} \\Sigma^{-1}(x-p_1)\\Big] \\Big] \\\\ \\hspace{-20mm} \\Big[- \\Big[- \\frac{k}{2}log(2\\pi) - \\frac{1}{2}log(|\\Sigma_2|) - \\frac{1}{2} \\Big[(x-p_2)^{T} \\Sigma^{-1}(x-p_2)\\Big]\\Big]\\Big] \\end{multline} $$  $$ $$\n $$ \\begin{multline} D_{KL}(p(x) || q(x)) = \\sum_x P(x) * \\\\ \\hspace{25mm} \\xcancel{-\\frac{k}{2}log(2\\pi)} \\boxed{- \\frac{1}{2}log(|\\Sigma_1|)} - \\frac{1}{2} \\Big[(x-p_1)^{T} \\Sigma^{-1}(x-p_1) \\\\ \\xcancel{+\\frac{k}{2}log(2\\pi)} \\boxed{+ \\frac{1}{2}log(|\\Sigma_2|)} + \\frac{1}{2} \\Big[(x-p_2)^{T} \\Sigma^{-1}(x-p_2)\\Big] \\end{multline} $$  We can cancel out the parts that give 0, club the boxed item in one and remaining terms in one. Hence,\n $$ \\begin{multline} D_{KL}(p(x) || q(x)) = \\sum_x P(x) * \\\\ \\hspace{25mm} \\Bigg[ \\hspace{5mm} \\boxed{+ \\frac{1}{2}log(\\frac{|\\Sigma_2|}{|\\Sigma_1|})} - \\frac{1}{2} \\Big[(x-p_1)^{T} \\Sigma^{-1}(x-p_1) \\Big] + \\frac{1}{2} \\Big[(x-p_2)^{T} \\Sigma^{-1}(x-p_2)\\Big]\\Bigg]\\tag{6} \\end{multline} $$  $P(x)$ can be split for each of the terms and hence can be written as:\n $$ \\begin{multline} D_{KL}(p(x) || q(x)) = \\sum_x P(x) * \\Bigg[ \\hspace{5mm} \\frac{1}{2}log(\\frac{|\\Sigma_2|}{|\\Sigma_1|})\\Bigg] \\\\ \\sum_x P(x) * \\Bigg[- \\frac{1}{2} \\Big[(x-p_1)^{T} \\Sigma^{-1}(x-p_1) \\Big] \\Bigg] + \\sum_x P(x) * \\Bigg[ \\frac{1}{2} \\Big[(x-p_2)^{T} \\Sigma^{-1}(x-p_2)\\Big]\\Bigg] \\tag{7} \\end{multline} $$  Now let us solve the remaining terms one by one. We will start with\n$$ \\sum_x P(x) * \\Bigg[ \\frac{1}{2} \\Big[(x-p_2)^{T} \\Sigma_2^{-1}(x-p_2)\\Big]\\Bigg] \\equiv E_p\\Bigg[ \\frac{1}{2} \\Big[(x-p_2)^{T} \\Sigma_2^{-1}(x-p_2)\\Big]\\Bigg] $$\nUsing properties of trace and expectation (Appendix -1), we can re-write:\n$$ E_p\\Bigg[ \\frac{1}{2} \\Big[(x-p_2)^{T} \\Sigma_2^{-1}(x-p_2)\\Big]\\Bigg] $$ as\n$$ = \\frac{1}{2}E_p\\Bigg[ tr\\Big[(x-p_2)^{T} \\Sigma_2^{-1}(x-p_2)\\Big]\\Bigg] = E_p\\Bigg[ tr\\Big[\\frac{1}{2} (x-p_2)^{T} \\Sigma_2^{-1}(x-p_2)\\Big]\\Bigg] = E_p\\Bigg[ tr\\Big[\\frac{1}{2}(x-p_2)(x-p_2)^{T} \\Sigma_2^{-1}\\Big]\\Bigg] $$\nUsing propery __, we have\n$$ = tr \\Bigg[\\hspace{3mm}\\boxed{E_p \\Bigg( \\Bigg[(x-p_2)(x-p_2)^{T} \\Bigg]} \\frac{1}{2} \\Sigma_2^{-1} \\Bigg) \\Bigg] $$\nThe boxed element is nothing but a covaiance matrix $\\Sigma_2$, therefore $$ D_{KL}\\Big(p(x) || q(x)\\Big) = tr \\Bigg[ \\Sigma_2 \\frac{1}{2} \\Sigma_2^{-1}] \\Bigg] = tr \\Bigg[ I_k \\Bigg] \\equiv k $$\n(Equations in Latex are so tough sometimes) To be continued \u0026hellip;\n","date":-27080281320,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-27080281320,"objectID":"373910c2fe5eae86a4a06eb0e660fd19","permalink":"https://srishti.dev/post/1111-11-11-kl-divergence/","publishdate":"1111-11-11T12:38:00-07:00","relpermalink":"/post/1111-11-11-kl-divergence/","section":"post","summary":"What is KL-Divergence KL Divergence is a measure of how one probabilty distributon is different from another. Some people also call it the distance between two distributions, however, strictly speaking it is not the distance.","tags":"technical","title":"KL-Divergence","type":"post"},{"authors":null,"categories":null,"content":"Question: If a class has only two samples, can a computer make correct prediction? Note: Number of samples is too less for training.\nApproach: Few Shot Learning\nFew shot learning is a problem where we try to learn when the training data is very small. It is different from supervised learning where we train on some data and try to predict an object which belongs to a class present in training data. In few shot learning, the training data has never the sample we are predicting on.\nLe\u0026rsquo;s take an example. Look at Fig.1\n  Fig 1. Photo via CS231 course   We train a model of a big training set of cats, dogs, mug and hat. The goal in few shot learning is not to recognize unseen cats/dogs/mugs/hats. Instead the goal is to recognize the similarity and difference between objects. After training, if we show two pairs of images (Fig.2 and Fig.3) to the model and ask \u0026ldquo;Are the two images similar?\u0026quot;.\n   * { box-sizing: border-box; } .column img { /* width: 100px; height: 150px; */ /* float: left ; */ padding: 10px; } /* Clearfix (clear floats) */ .row::after { content: \"\"; clear: both; display: table; }     Fig 2. Photo via Medium blog    Fig 3. Photo via Medium blog      Since the model has learned the similarity and difference between objects, it can tell that images in Fig.2 are same kind of objects and Fig.3 are quite different. Now, if you ask the model to recognize the objects, it does not know it is cat or dog because it isn\u0026rsquo;t a part of the training data. Hence, model can tell that these two images are similar but can\u0026rsquo;t tell what they are.\nNow, let\u0026rsquo;s ask another question. Let\u0026rsquo;s ask the model what is Fig. 4. We call this image: Query\n  Fig 4. \"Query\" (Photo via web)   Model is unable to answer this question because it has never seen this data during training. Now, I show another 4 images to the model as shown in Fig. 5\n   Fig 5. \"Support-Set\"   Now, model compares the query image with each image in the support set and believes the query is \u0026ldquo;Squirre\u0026rdquo;. These set of labelled images are called \u0026ldquo;Support Set\u0026rdquo;.\n Jargons related to few-shot learning\n k-way: we are given $j$ classes (e.g. 3-way means 3 classes. See Fig.1) n-shot: the number of samples per class (e.g. 2-shot mean 2 sample per class. See Fig.3) Support set($S$): the samples used for learning. If we have $j$ classes and $k$ samples, then number of elements in support set is $ j * k $. Query set ($Q$): the sample(s) which we are trying to identify.    Fig 3. Photo via Borealis AI blog   More formally,if we define task with $T$, then task $T_i$ is sampled from probabilty distribution over tasks\n $$ T_i\\sim P(T) $$ and each task $i$ is a set given by Support $S$ and Query $Q$ $$ T_i = \\{ S_i, Q_i \\} $$ ","date":-27080281320,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-27080281320,"objectID":"e3a30586cccffc31f9e877ef35749837","permalink":"https://srishti.dev/post/1111-11-11-low-shot-learning-basic-concepts/","publishdate":"1111-11-11T12:38:00-07:00","relpermalink":"/post/1111-11-11-low-shot-learning-basic-concepts/","section":"post","summary":"Question: If a class has only two samples, can a computer make correct prediction? Note: Number of samples is too less for training.\nApproach: Few Shot Learning\nFew shot learning is a problem where we try to learn when the training data is very small.","tags":"technical","title":"Low/Few Shot Learning","type":"post"},{"authors":null,"categories":null,"content":"Paper: Link\nObjective: \n Explores how can we use additional meta-data available to make better classification (in this case animal species). Explores how to make best use of additional meta data which comes with most images today.  Summary: \n Knowing where a given image was taken can provide a strong prior for what objects it may contain. Paper provide a novel training loss to capture these relationship The data they assemble can have unrelated image and location dataset as long as both contain the same categories. At test time, given an image and where and when it was taken, they aim to estimate which category it contains. Location information is incorporated as bayesian spatio-temporal prior. Also, during the modelling, spatio temporal (longitude, latitude, time) are independent from the image classifier It is difficult and time consuming to have information on where and when a given category has been a.) observed to be present and b.) observed to be absent. Hence, the paper explores presence-only setting (novelty)  ","date":-27080281320,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-27080281320,"objectID":"bb0b972c73481f9e36dfa1cddbfb0cf0","permalink":"https://srishti.dev/post/1111-11-11-presence-only-geographical-priors-for-fine-grained-image-classification/","publishdate":"1111-11-11T12:38:00-07:00","relpermalink":"/post/1111-11-11-presence-only-geographical-priors-for-fine-grained-image-classification/","section":"post","summary":"Paper: Link\nObjective: \n Explores how can we use additional meta-data available to make better classification (in this case animal species). Explores how to make best use of additional meta data which comes with most images today.","tags":"technical","title":"Presence-Only Geographical Priors for Fine-Grained Image Classification","type":"post"},{"authors":null,"categories":null,"content":"Paper: Link\nObjective: \n Focuses on species of plants and animals captured in wide variety of situations, different camera types, varying image quality, feature large class imbalance and verified by citizen scientists.  Summary: \n  Details: There are a total of 5,089 categories in the dataset, with 579,184 training images and 95,986 validation images. For the training set, the distribution of images per category follows the observation frequency of that category by the iNaturalist community. Therefore, there is a non-uniform distribution of images per category.\n  Experiments:  Classification experiments were done using ResNet, Inception V3, Inception ResNet V2 and MobileNet\n  Known issues:  a.) Doesn\u0026rsquo;t contains additional annotations such as sex and life stage attributes, habitat tags, and pixel level labels for the four super-classes that were challenging to annotate. b.) Need of an efficient algorithm that works when the test set contains classes that were never seen during training.\n  ","date":-27080281320,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-27080281320,"objectID":"09e9468ee9fa00f02ddda5e9294e7fde","permalink":"https://srishti.dev/post/1111-11-11-the-inaturalist-species-classification-and-detection-dataset/","publishdate":"1111-11-11T12:38:00-07:00","relpermalink":"/post/1111-11-11-the-inaturalist-species-classification-and-detection-dataset/","section":"post","summary":"Paper: Link\nObjective: \n Focuses on species of plants and animals captured in wide variety of situations, different camera types, varying image quality, feature large class imbalance and verified by citizen scientists.","tags":"technical","title":"The iNaturalist Species Classification and Detection Dataset","type":"post"},{"authors":null,"categories":null,"content":"Paper: Link\nObjective:  Leverage free, noisy data from the web to train effective models of fine-grained recognition.\nSummary: \n Interesting paper on using noisy data from the web. They sample images directly from Google search, using all returned images as images for a given category. For L-Bird and L-Butterfly, queries are for the scientific name of the category, and for L-Aircraft and L-Dog queries are simply for the category name (e.g.“Boeing 737-200” or “Pembroke Welsh Corgi”). Active learning-based approach to collect the data. The active learning begins by training a classifier on a seed set of input images and labels (i.e.the Stanford Dogs training set), then proceeds by iteratively picking a set of images to annotate, obtaining labels with human annotators, and re-training the classifier. Inception V3 is the base classifier To avoid images overlap between GT and web images, aggressive duplication procedure with all ground truth test sets and their corresponding web images is performed using a SOTA for learning similarity metric between images.  Questions: \n How reliable are the search results for a single category from web, otherwise there are a lot of False Positives being introduced. Is it limited to extracting category information or do we need extra labels like position, time etc. How much human annotators are required to be involved or are they even involved? Since algo queries user to label data from time to time; does this query apply on a subset of the images to verify the annotations or create annotations.  ","date":-27080281320,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-27080281320,"objectID":"6e2b15c16c3f33f926b32da974f3b5ce","permalink":"https://srishti.dev/post/1111-11-11-the-unreasonable-effectiveness-of-noisy-data-for-fine-grained-recognition/","publishdate":"1111-11-11T12:38:00-07:00","relpermalink":"/post/1111-11-11-the-unreasonable-effectiveness-of-noisy-data-for-fine-grained-recognition/","section":"post","summary":"Paper: Link\nObjective:  Leverage free, noisy data from the web to train effective models of fine-grained recognition.\nSummary: \n Interesting paper on using noisy data from the web. They sample images directly from Google search, using all returned images as images for a given category.","tags":"technical","title":"The Unreasonable Effectiveness of Noisy Data for Fine-Grained Recognition","type":"post"},{"authors":null,"categories":null,"content":"General Terms:  Semantic: Semantics is the study of relationship between words and how we can draw meaning from words. Example, A child could be called a child, kid, boy, girl, son, daughter. Sematic relations connect up entities in a text. Semantic relation are at the cross-road between knowledge and language and tTogether with entities make up good chunk of the meaning of the text.\nTo explore, how sets of group of words are linked by means of sevaeral semantic relations, one can look at WordNet.\n Gist:  For humans it is easy to identify how two events are related by looking at them. Inherently, we can decompose two events in general abstract meaning and see which of the abstract meaning are similar. Example, when we see a.) a human being typing on a computer b.) a GIF with a mouse typing on a calculator, we can relate the hand movements and object being typed upon. We can relate that both activities are related to pushing some buttons and seeing a result on screen and hence can see the similarities in the two activities. It is a result of underlying human decision making abilty. Computers can\u0026rsquo;t do the same.\nThe paper proposes \u0026ldquo;an approach for learning semantic relational set abstractions on videos\u0026rdquo;. \u0026ldquo;semantic relational set abstractions\u0026rdquo; should be read in two parts \u0026ldquo;semantic\u0026rdquo; and \u0026ldquo;relational set abstractions\u0026rdquo; for better understanding. It simply says that if we can find the relational sets between different videos (e.g. similar images at different view angles) and get an abstract meaning from these relational sets, the proposed method can learn the semantic (meaning) between these abstractions. If we can do so, is will be easier to tell which images are similar, which are different and how they are similar/different.\nMore formally, as the paper puts it, this allows our model to perform cognitive tasks such as:\n set abstraction (which general concept is in common among a set of videos?) set completion (which new video goes well with the set?), and odd one out detection (which video does not belong to the set?)  Datsets used: \n K400 (Kinect 400) Multi-Monets in Time  ","date":-27080281320,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-27080281320,"objectID":"3ecd069b8107b1670103aa7d2f79acf7","permalink":"https://srishti.dev/post/1111-11-11-we-have-so-much-in-common/","publishdate":"1111-11-11T12:38:00-07:00","relpermalink":"/post/1111-11-11-we-have-so-much-in-common/","section":"post","summary":"General Terms:  Semantic: Semantics is the study of relationship between words and how we can draw meaning from words. Example, A child could be called a child, kid, boy, girl, son, daughter.","tags":"technical","title":"We Have So Much In Common: Modeling Semantic Relational Set Abstractions In Videos","type":"post"},{"authors":null,"categories":null,"content":"If you are using Ubuntu subshell in Windows (I use the one available in Microsoft Store), this is for you. There are times, where you may need to access the files saved on your windows in the Linux subshell. All you need to do is mount your drive. You may need to access the subshell via administrative privileges. For example, if you have the file in C: Drive, use the following:\ncd /mnt/c ls  The same can be used to access data from external hard disk too. If the external disk is in D: Drive, the command would simply change to:\ncd /mnt/d ls  ","date":-27080321694,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-27080321694,"objectID":"a69e5b0b2e2ba5bfe6a6e76ea5d8280f","permalink":"https://srishti.dev/post/1111-11-11-accessing-windows-from-linux/","publishdate":"1111-11-11T01:25:06-07:00","relpermalink":"/post/1111-11-11-accessing-windows-from-linux/","section":"post","summary":"If you are using Ubuntu subshell in Windows (I use the one available in Microsoft Store), this is for you. There are times, where you may need to access the files saved on your windows in the Linux subshell.","tags":"technical","title":"Accessing Windows File From Linux Subshell in Windows","type":"post"},{"authors":null,"categories":null,"content":" Project 1 CLIP-based Predictive Domain Adaptation Work in progress: Code here\nProject 2 Design, Implementation and Delivering of a ML System for Cloud,Shadow and Haze Detection from Satellite Data. Goal: Given unprocessed geospatial data from standard sources, aim was to detect and predict cloud, shadow and haze cover.\nThis project involved formatting raw data to match a standard specification and then used as an input to a deep learning-based image segmentation model. The entire software stack was cloud based and AWS EC2 was used the main development server. Docker was used for creating virtual environments and parallel deployment of multiple training jobs. Jupyter notebooks were used as a mechanism for integrating both code and documentation into a single deliverable medium.\nEffort was aimed at developing machine learning models which would suit our purpose. This involved data augmentation, data filtering, reiteration with different hyper-parameters, improving code to incorporate added functionality and implementing the solution using standardized software packages and frameworks like PyTorch.\nProject 3 Optimization Methods for Better Cloud Mask from Sentinel 2 Data Goal:Scientific understanding of how radiometric properties can affect cloud detection.\nA standard cloud detection approach for visible bands will model the threshold value as a linear function with respect to reflectance. We did a linear transformation between our Top-of-Atmosphere (TOA) reflectance values (band reflectance) to the NBAR albedo reflectance values (reference reflectance). The idea was to use this approximate correction to make the threshold more accurate to generate clearer cloud masks.\n   Sample Sentinel 2 Cloud Mask.    Note: To do basic exploration with geospatial data and understand how it works and what it looks like, see the code here: https://github.com/copperwiring/3-band-satellite-imagery-analysis . It is recommended to use QGIS to view TIF files when needed.\nProject 4 Microsoft Kinect RGB-Depth Based Visual Tracking The project (part of my Masters thesis) addresses the challenge of re-detection of a single occluded target. This work specifically focuses on human targets that are occluded by objects (e.g. chairs) or by other humans (e.g. a tall person). The tracking methodology proposed considers a single Kinect RGB-D camera, single-target,and is model-free, applied to long-term tracking. The model-free property means that the only supervised training example is provided by the bounding box in the first frame. The long-term tracking means that the tracker learns to re-detection after the target is lost. i.e. to infer the object position in the current frame.\n   Sample RGB and Depth images which were used in the process.    Click the image below to see the video of the final results.\n \n -- Project 4 Image Classification Model Deployment Using Flask This project builds a image classification model which recognises \u0026lsquo;House Numbers\u0026rsquo; (in photos). It uses Flask to create an API, we can deploy this model and create a simple web page to load and classify new images.\nThe model can be tested on website: srishti08.pythonanywhere.com\nNote: To test the model, please download sample images from here: 1, 2 and 3 or you can download all the images from here.\n    Code \u0026amp; steps to reproduce on GitHub: https://github.com/copperwiring/model-deployment-locally-and-with-flask\n","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"6087c0ef875554f4409ac52928d79279","permalink":"https://srishti.dev/projects/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/projects/","section":"","summary":"Project 1 CLIP-based Predictive Domain Adaptation Work in progress: Code here\nProject 2 Design, Implementation and Delivering of a ML System for Cloud,Shadow and Haze Detection from Satellite Data. Goal: Given unprocessed geospatial data from standard sources, aim was to detect and predict cloud, shadow and haze cover.","tags":null,"title":"Projects","type":"page"}]