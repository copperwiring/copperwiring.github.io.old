<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>technical | Srishti Yadav</title>
    <link>https://srishti.dev/tag/technical/</link>
      <atom:link href="https://srishti.dev/tag/technical/index.xml" rel="self" type="application/rss+xml" />
    <description>technical</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><lastBuildDate>Sun, 12 Nov 1111 22:21:06 -0700</lastBuildDate>
    <image>
      <url>https://srishti.dev/images/icon_hu83f24c6794b0c645cb0fb9e8b29ab31d_111941_512x512_fill_lanczos_center_2.png</url>
      <title>technical</title>
      <link>https://srishti.dev/tag/technical/</link>
    </image>
    
    <item>
      <title>On The Effects of Learning Views on Neural Representations in Self-Supervised Learning</title>
      <link>https://srishti.dev/post/1111-11-11-viewmaker-network-work-but-why/</link>
      <pubDate>Sun, 12 Nov 1111 22:21:06 -0700</pubDate>
      <guid>https://srishti.dev/post/1111-11-11-viewmaker-network-work-but-why/</guid>
      <description>&lt;p&gt;Update: A small section of latex in the LaTeX is not rendering correctly. Will fix it soon.&lt;/p&gt;
&lt;p&gt;A blog post on 
&lt;a href=&#34;https://iclr.cc/virtual/2021/poster/2544&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;em&gt;Viewmaker Networks: Learning Views for Unsupervised Representation Learning&lt;/em&gt;&lt;/a&gt;. ICLR, 2021&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;About this Blog Post&lt;/strong&gt;:&lt;/p&gt;
&lt;p&gt;Contrastive self-supervised learning (SSL) methods require domain expertise to select a set of data transformations that work well on a specific downstream task, modality, and domain. This creates a bottleneck for the utilization of SSL strategies on new datasets, particularly those sampled from different domains. This blog post details the Viewmaker Networks, a method that attempts to overcome this issue by using constrained perturbations and an adversarial training objective to synthesize novel views. This method can be broadly extended to many domains, as it learns views directly from the training data. We cover the details of the methodology and related concepts as they apply to contrastive visual representation learning. This blog post also develops further insights into the visual representations learned using the Viewmaker&amp;rsquo;s augmented views by conducting additional experiments. Specifically, we investigate the dimensional collapse and representational similarity between differently trained models (handcrafted views and viewmaker views). We observe that training models using Viewmaker views not only make better use of the embedding space but also learn a different function of the data when compared to training models using handcrafted views, something one should carefully consider when choosing one over the other. We feel these observations will encourage further discussions on learned views in self-supervised learning.&lt;/p&gt;
&lt;center&gt;
&lt;figure&gt;
&lt;img align=&#34;centre&#34; src=&#34;https://i.imgur.com/SmmBsnK.gif&#34; alt=&#34;Trulli&#34; style=&#34;width:100%&#34;&gt;
&lt;figcaption style=&#34;text-align:center&#34;&gt;
    Fig.1:  The Viewmaker Network Setup
&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;/center&gt;
&lt;div&gt; 
&lt;blockquote&gt;
  &lt;p&gt;
Contrastive self-supervised learning(SSL) methods require domain expertise to select a set of data transformations that work well on a specific downstream task, modality, and domain. This creates a bottleneck for the utilization of SSL strategies on new datasets, particularly those sampled from different domains. This blog post details the Viewmaker Networks, a method that attempts to overcome this issue by using constrained perturbations and an adversarial training objective to synthesize novel views. This method can be broadly extended to many domains, as it learns views directly from the training data. We cover the details of the methodology and related concepts as they apply to contrastive visual representation learning.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;!-- ## **Motivation** --&gt;
&lt;p&gt;The amount of data available to us is exploding. Despite the availabilty of such large amounts of data, we are currently unable to exploit it because the majority of existing deep learning research relies on annotated data. We require a learning method which can use training data with fewer or no labels and yet perform well when identifying patterns in unseen data. This learning paradigm, which doesn&amp;rsquo;t assume availabilty of labelled training data, is called Unsupervised Learning. We specifically explore a subset of unsupervised methods, which utilizes pretext tasks that develop a training objective which exploits the structure in the data itself. This subset of Unsupervised Learning is called Self-Supervised Learning(SSL).&lt;/p&gt;
&lt;p&gt;Self-Supervised learning (SSL) requires a human expert to design a pretext task. Recently, constrastive learning has emerged as an effective and popular pretext task. It works by extracting a weak supervisory signal from carefully chosen data transformations, typically specified by a domain expert. This creates a barrier for the use of such models by non-domain experts as well as issues with usabilty across various domains and modalities. Viewmaker Network attempts to remove this barrier by proposing a network which is domain agnostic and yet generates useful learned views (i.e. augmented images) which can be used for model training and show significant success on a variety of classification tasks across various datasets and modalities. This blog post also develops further insights into the visual representations learned using the Viewmaker&amp;rsquo;s augmented views by conducting additional experiments. Specifically, we investigate the dimensional collapse and representational similarity between differently trained models (handcrafted views and viewmaker views). We observe that training models using Viewmaker views not only make better use of the embedding space but also learn a different function of the data when compared to training models using handcrafted views, something one should carefully consider when choosing one over the other. We feel these observations will encourage further discussions on learned views in self-supervised learning.&lt;/p&gt;
&lt;h1 id=&#34;1-representation-learning&#34;&gt;1. Representation Learning&lt;/h1&gt;
&lt;!-- One of the basic concepts of the Viewmaker Network is  &#39;views&#39; (Section 2) or what we can also call image transformations. These image &#39;representations&#39; in deep learning are non-linear transformations of the data that are meaningful and useful for downstream tasks when building predictive models.  --&gt;
&lt;p&gt;The task of self-supervised representation learning requires discovering ways to learn abstract representations from the data, which capture relevant high-level properties, without any explicit annotations. Recently, representation learning has seen extensive successful usage in deep learning, with the goal of finding more abstract – and ultimately more useful representations.&lt;/p&gt;
&lt;p&gt;For visual representations to be useful, they must satisfy a set of criteria. Firstly, they must encode useful semantic properties that explain the image contents. Secondly, the learned representations should be useful for performing downstream image analysis tasks (such as classification). And thirdly, they must be robust to various changes to the image, such as natural variations in lighting or viewpoint.&lt;/p&gt;
&lt;p&gt;There are multiple strategies for learning visual representations. When learning visual representations in a supervised manner, the resulting representations are often specific to the supervised task and annotations. This is undesirable, as these representation may not generalize to different tasks i.e. they are not task and label agnostic. One strategy for mitigating these issues and learning better visual representation is to shift to a self-supervised learning approach. SSL approaches do not rely on annotations and are agnostic to downstream tasks, and so these representations might be more general.&lt;/p&gt;
&lt;h1 id=&#34;2-views&#34;&gt;2. Views&lt;/h1&gt;
&lt;h2 id=&#34;21-what-are-views&#34;&gt;2.1 What Are Views?&lt;/h2&gt;
&lt;!-- About Self-Supervised Visual Representation Learning, Hand-crafted views, recent works on automatizing the process, etc. --&gt;
&lt;!-- Read this again --&gt; 
&lt;figure&gt;
&lt;img src=&#34;https://i.imgur.com/JFGTXRy.png&#34; alt=&#34;Trulli&#34; style=&#34;width:100%&#34;&gt;
&lt;figcaption style=&#34;text-align:center&#34;&gt; 
    Fig.2: Different views of a Labrador Retriever. The left most images are natural views sampled by taking different photographs of a Labrador Retriever. The rightmost images are synthetic views sampled by modifying a single natural view.
&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;Many natural images contain visual representations of semantically distinct objects such as animals, plants, furniture, etc. Given an image containing an instance of some object class, this image is only one potential view of that object. For example, in Figure 2, there are two unique images of a Labrador Retriever photographed at different times and from different locations. These are examples of natural views of an object, which are essentially just the set of all possible photographs of that object as it might exist in the real world. Figure 2 also portrays two synthetic views of the Labrador Retriever. These are synthetically augmented images, which modify the image while still retaining the relevant semantic information. Similar to natural views, these are valid visual representations of the object. However, they are derived by modifying a single image from the set of natural views using various augmentation strategies such as geometric transformations of an image or color modification.&lt;/p&gt;
&lt;figure&gt;
&lt;img src=&#34;https://1.bp.blogspot.com/-bO6c2IGpXDY/Xo4cR6ebFUI/AAAAAAAAFpo/CPVNlMP08hUfNPHQb2tKeHju4Y_UsNzegCLcBGAsYHQ/s1600/image3.png&#34; alt=&#34;Trulli&#34; style=&#34;width:100%&#34;&gt;
&lt;figcaption style=&#34;text-align:center&#34;&gt; 
    Fig.3: SimCLR data augmentation options. The authors included geometric transformations like cropping, rotation, resizing, and flipping; and color transformations like blurring, and color jitter.
&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;!-- https://europe.naverlabs.com/research/computer-vision/learning-visual-representations/ --&gt;
&lt;h2 id=&#34;22-which-views-are-useful&#34;&gt;2.2 Which Views Are Useful?&lt;/h2&gt;
&lt;!-- The fact that views must be designed by hand is a significant limitation. --&gt;
&lt;p&gt;Many previous works have achieved significantly improved model performance by proposing a variety of image augmentation strategies. Historically, it was the development of the AlexNet architecture (&lt;strong&gt;ImageNet Classification with Deep Convolutional Neural Networks&lt;/strong&gt;) which popularized the use of augmentations in deep learning. The transformations they considered involved randomly cropped 224×224 patches from the original images, flipping them horizontally, and changing the intensity of the RGB channels using PCA color augmentation. Since then, various novel augmentation strategies have demonstrated effectiveness in various domains. Random Erasing (&lt;strong&gt;Random Erasing Data Augmentation&lt;/strong&gt;) is an augmentation strategy that attempts to mimic the natural object occlusion which occurs in many image modalities. This strategy, demonstrated in Fig. 4, involves randomly selecting a rectangular image region and replacing the pixel values with random values or a single pixel value. Related to this approach (&lt;strong&gt;Improved Regularization of Convolutional Neural Networks with Cutout&lt;/strong&gt;) is CutOut, a procedure that involves randomly masking out square regions of the input during training. The authors find that cutout improves the robustness and overall performance of convolutional neural networks. Another augmentation strategy, known as Mixup, (&lt;strong&gt;mixup: Beyond Empirical Risk Minimization&lt;/strong&gt;) involves augmenting both the input image and the image label. Mixup proceeds by sampling two images from the dataset and interpolating between them by some factor. Then, the one-hot encoded image labels are interpolated by the same factor. Yun et. al. proposed CutMix (&lt;strong&gt;CutMix: Regularization Strategy to Train Strong Classifiers with Localizable Features&lt;/strong&gt;), an augmentation strategy that combines CutOut and Mixup. CutMix proceeds by applying CutOut to an input image and then replaces the cutout region with a cropped section from another image. Then, the label is augmented such that it represents the percentage of the image occupied by each object class. DAGAN (&lt;strong&gt;Data Augmentation Generative Adversarial Network&lt;/strong&gt;) utilizes an adversarial setup to learn image augmentations and the authors showed that adversarial training can be used to expand the pool of available data and even apply novel augmentations to unseen classes of data (e.g. few-shot learning). This field of optimizing best augmentation strategies is constantly expanding.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th align=&#34;center&#34;&gt;Black&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;White&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Random&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://i.imgur.com/u4IS2EJ.gif&#34; alt=&#34;i1&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://i.imgur.com/SWrTAvd.gif&#34; alt=&#34;i2&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://i.imgur.com/0elggeF.gif&#34; alt=&#34;i3&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://imgur.com/LFfityQ.gif&#34; alt=&#34;i4&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://imgur.com/JN5aVIY.gif&#34; alt=&#34;i5&#34;&gt;&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;&lt;img src=&#34;https://imgur.com/8y55KTi.gif&#34; alt=&#34;i6&#34;&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;figcaption style=&#34;text-align:center&#34;&gt;
    Fig.4: Random Erasing Augmetation Strategy
&lt;/figcaption&gt;
&lt;center&gt;
&lt;figure&gt;
&lt;img align=&#34;centre&#34; src=&#34;https://i.imgur.com/2mC2AjM.jpg&#34; alt=&#34;Trulli&#34; style=&#34;width:100%&#34;&gt;
&lt;figcaption style=&#34;text-align:center&#34;&gt;
    Fig.5: (left to right) Mixup, Cutout and CutMix data augmentations
&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;/center&gt;
&lt;p&gt;The intuition behind using many of the common image transformations is that these augmented views attempt to simulate samples from the true underlying data distribution. This improves the diversity of the available data and can help improve model convergence. This rationale, however, does not explain why unrealistic distortions such as cutout (&lt;strong&gt;Improved Regularization of Convolutional Neural Networks with Cutout&lt;/strong&gt;) and mixup (&lt;strong&gt;mixup: Beyond Empirical Risk Minimization&lt;/strong&gt;) significantly improve generalization. Furthermore, many augmentation strategies do not always transfer across datasets. Cutout, for example, improves performance on CIFAR-10 but not on ImageNet(&lt;strong&gt;Improving Robustness Without Sacrificing Accuracy with Patch Gaussian Augmentation&lt;/strong&gt;). There are also no standard metrics that quantify if an augmentation strategy is good and how well can it be generalized because they are model-dependent. Viewmaker Network aims to remove this dependency on handcrafted views by learning image transformations from the data.&lt;/p&gt;
&lt;h1 id=&#34;3-where-does-viewmaker-network-fit-in&#34;&gt;3. Where Does Viewmaker Network Fit In?&lt;/h1&gt;
&lt;p&gt;Recently, contrastive self-supervised representation learning methods have explored training models which are invariant to different “views,” or augmented versions of an input. However, designing these views requires considerable trial and error by human experts, hindering its widespread adoption across domains and modalities thus leading to the question: how can we better transform image with minimum human expertise?&lt;/p&gt;
&lt;p&gt;Images can be transformed in a vast number of ways. However, only a subset of these transformations provide a useful training signal which can be exploited by contrastive methods. Within this subset of informative transformations, only a few are ever selected by experts. As seen in Fig. 6, there is a vast untapped space of image transformations which can be used for contrastive methods. The Viewmaker Network attempts to explore this transformation space, denoted as &lt;strong&gt;B&lt;/strong&gt;, using an adversarial setup without any input from a human expert.&lt;/p&gt;
&lt;center&gt;
&lt;figure&gt;
&lt;img align=&#34;centre&#34; src=&#34;https://i.imgur.com/8HoEHkv.png&#34; alt=&#34;The set of image transformations&#34;&gt;
&lt;figcaption style=&#34;text-align:left&#34;&gt;
    Fig.6: Possible sets of image transformations where &lt;b&gt;A&lt;/b&gt; = The set of all image transformations; &lt;b&gt;B&lt;/b&gt; = the set of all transformations that provide a useful training signal for contrastive methods; &lt;b&gt;C&lt;/b&gt; = the set of handcrafted transformations. 
&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;/center&gt;
&lt;!--DONT REMOVE THIS COMMENT /public/images/cifar-result.png --&gt;
&lt;h1 id=&#34;4-learning-to-generate-views&#34;&gt;4. Learning to Generate Views&lt;/h1&gt;
&lt;p&gt;Using neural networks to learn image transformations for generating augmented views is not a simple task, as we have no annotated training data that explicitly indicates whether or not a network augmented image is within the set of valid views. An alternative strategy is to utilize an adversarial set up such that one model can guide the other into learning transformations that produce valid views. In an adversarial training setup, we have two neural networks: the generator $G$ and the discriminator $D$. The discriminator is typically tasked with receiving inputs from a real distribution, and a synthetic distribution, and then classifying which distribution each input originated from. The generator is typically optimized to generate an input that can fool the discriminator into believing the input originates from the real distribution. More formally, the discriminator is optimized to maximize the following objective function:&lt;/p&gt;
&lt;p&gt;$$\mathbb{E}_x[\texttt{log}D(x)] + \mathbb{E}_z[\texttt{log}(1-D(G(z)))]$$&lt;/p&gt;
&lt;p&gt;And the generator is optimized to minimize the following objective function:&lt;/p&gt;
&lt;p&gt;$$\mathbb{E}_z[\texttt{log}(1-D(G(z)))]$$&lt;/p&gt;
&lt;p&gt;where $x$ is an input sampled from the real distribution, and $z$ is a stochastic input that the generator modifies to produce samples that match the real distribution, $D$ is the discrimantor and $G$ is the generator.&lt;/p&gt;
&lt;p&gt;The adversarial setup used for training the Viewmaker Networks differs from traditional adversarial training setups in a few distinct ways. First, there is no real distribution. Second, the discriminator is optimized using an instance discrimination objective, such as infoNCE (&lt;strong&gt;Representation Learning with Contrastive Predictive Coding&lt;/strong&gt;), which involves the task of re-identify a transformed version of some instance of the image $x_i$ within a set of many other different images. Third, the generator is tasked with modifying an image as follows:&lt;/p&gt;
&lt;p&gt;$$x_{view} = G(x_i) + x_i$$&lt;/p&gt;
&lt;p&gt;such that the new image $x_{view}$ maximizes the instance discrimination objective. This setup encourages $D$ to learn representations of images that allow it to re-identify modified images, and it encourages $G$ to learn transformations of $x_i$ such that it is becomes more difficult to distinguish from unrelated images. However, this method has a major failure case &amp;ndash; if the generator is allowed to arbitrarily modify $x_i$, then it can learn a solution where it simply destroys all revevant semantic information in $x_i$. Thus, this method requires that a constraint is applied to the transformations learned by $G$ such that it cannot converge to this undesired solution.&lt;/p&gt;
&lt;!-- Source: https://stackoverflow.com/questions/24319505/how-can-one-display-images-side-by-side-in-a-github-readme-md  --&gt;
&lt;centre&gt;
&lt;figure&gt;
&lt;p align=&#34;middle&#34;&gt;
  &lt;img src=&#34;https://i.imgur.com/KUNRWj7.png&#34; alt=&#34;Trulli&#34; width=&#34;30%&#34;/&gt;
  &lt;img src=&#34;https://i.imgur.com/XUHB4Or.png&#34; alt=&#34;Trulli&#34; width=&#34;30%&#34; /&gt;
  &lt;img src=&#34;https://i.imgur.com/QaUJDp3.png&#34; alt=&#34;Trulli&#34; width=&#34;30%&#34;/&gt;
&lt;figcaption style=&#34;text-align:center&#34;&gt;
    Fig.6. Sample Views from Viewmaker Network at 200th epoch 
&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;/p&gt;
&lt;/centre&gt;
&lt;h2 id=&#34;51-making-novel-views-with-adversarial-perturbations&#34;&gt;5.1 Making Novel Views with Adversarial Perturbations&lt;/h2&gt;
&lt;!-- Reword below
 --&gt;
&lt;p&gt;The method proposed by the authors is insipired from adverserial setup, in particular with the use of $l_p$ norms used for adverserial robustness. But what is adversarial robustness and how is it related to samples of our data?&lt;/p&gt;
&lt;p&gt;Szegedy et al. and Biggio et al. showed that specifically crafted small perturbations of benign inputs can lead machine-learning models to misclassify them. These perturbed inputs are referred to as adversarial samples. Given a machine-learning model, a sample $\hat{x}$ is considered as an adversarial example if it is similar to a benign sample $x$ (drawn from the data distribution), such that $x$ is correctly classified and $\hat{x}$ is classified differently than x. (&lt;strong&gt;On the Suitability of Lp-norms for Creating and Preventing Adversarial Examples&lt;/strong&gt;)&lt;/p&gt;
&lt;h2 id=&#34;51-l_p-constrainted-perturbations&#34;&gt;5.1 $l_p$ Constrainted Perturbations&lt;/h2&gt;
&lt;p&gt;To explain the $l_p$ contrained perturbations proposed by the authors, we first need to define $l_p$ norm.&lt;/p&gt;
&lt;p&gt;For a real number $p \geq 1$, we can define the $l_p$ norm as $||\textbf{w}||_p = (∑_{i}^{n}|w_i|^p)^{1/p}$. Note that we use bold to denote vector notations. Based on the value of $p$, we get the $l_1$, $l_2$, .. , $l_n$ norms (**may be reference which has good details on lp norms?**)&lt;/p&gt;
&lt;!-- ~~Note that $l_p$ for $p=2$ and $n=2$ is a  $||\textbf{w}||_2 = \sqrt{w_1^2+w_2^2}$ which is a circle.~~ For a bettre mathematical intuition, one way would be to think about these norms it is to look at what happens to the unit circle as $p$ varies. When $p=∞$, the unit circle is just a square. As p decreases, the outer corners begin to move inward until $p=1$, when they become just straight lines between the cardinal unit vectors, and the unit circle becomes a diamond. For $p&lt;1$, the diagonal lines start to bulge inward, creating a four-pointed &#34;star&#34;.  --&gt;
&lt;!-- Source https://www.physicsforums.com/threads/intuitive-explanation-of-what-a-p-norm-is-for-any-arbitrary-p-0.520225/ --&gt;
&lt;p&gt;But how does $l_p$ constraints generate perturbations which are helpful for viewmaker network? To explain this we will refer to  Fig.7 (pseudo code for viewmaker network) and Fig. 8 which is its representation in an (assumed) 3D space.&lt;/p&gt;
&lt;figure&gt;
&lt;img src=&#34;https://i.imgur.com/eUwGsrF.png&#34; alt=&#34;Trulli&#34; /&gt;
&lt;figcaption style=&#34;text-align:center&#34;&gt;
    Fig.7.Psuedo Code for Viewmaker Network 
&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;centre&gt;
&lt;figure&gt;
&lt;img src=&#34;https://i.imgur.com/xyXpZJK.png&#34; alt=&#34;Trulli&#34; style=&#34;width:800%&#34;&gt;
&lt;figcaption style=&#34;text-align:center&#34;&gt;
    Fig.8: Representation of adding perturbation to input data in a sample 3D space &lt;/figcaption&gt;
&lt;/figure&gt;
&lt;/centre&gt;
&lt;!-- 
~~Referring to Fig. 8, let us assume a 3D space with centre at point 4. This space defines the region where images can exist. By definition, $L_1$ norm is $||\textbf{w}||_1 = ∑_{i}^{n}|w_i|$ and hence can be represented as the greenish pyramid structure centred at point 4 and 5 respectively. $L_2$ norm is $||\textbf{w}||_2^2 = ∑_{i}^{n}w_i^2$ and can be represented by the pink and yellow spheres centred around point 4 and 5 respectively.~~
 --&gt;
&lt;p&gt;Let us assume a 3D space with origin at point 1 (see Fig.8 above). We are working with 3D data points in this example scenario. Let 5 be an input data point in this space and 1 be a generated perturbation in this space. If we were to add points 1 and 5 to perturb the input, there is a very high chance that in such a blind addition, the features of the perturbation can completely overshaddow the features of the input data point. To avoid this, we need a constraint on the perturbation before adding it to the input.&lt;/p&gt;
&lt;p&gt;${l}_p$ normIf we constrain 1 using the&lt;/p&gt;
&lt;p&gt;By definition, $L_1$ norm is $||\textbf{w}||_1 = ∑_{i}^{n}|w_i|$ and hence can be represented as the greenish pyramid structure centred at point 4 and 5 respectively. $L_2$ norm is $||\textbf{w}||_2^2 = ∑_{i}^{n}w_i^2$ and can be represented by the pink and yellow spheres centred around point 4 and 5 respectively.&lt;/p&gt;
&lt;p&gt;To explain how a perturbation can be constrained by  Viewmaker Network creates image transformation, we will, for simplicity, assume our input data in some 3D space at point 5. Other descriptions of the points in the Fig.8 can be seen below followed by their use in the viewmaker network.&lt;/p&gt;
&lt;!-- (to add as legends in fig itself)
 --&gt;
&lt;p&gt;Point &lt;span style=&#34;color:red&#34;&gt;1&lt;/span&gt;: Unconstrained perturbation
Point &lt;span style=&#34;color:red&#34;&gt;2&lt;/span&gt;: Perturbation constrained by $l_2$ norm
Point &lt;span style=&#34;color:red&#34;&gt;3&lt;/span&gt;: Perturbation constrained by $l_1$ norm
Point &lt;span style=&#34;color:red&#34;&gt;4&lt;/span&gt;: origin
Point &lt;span style=&#34;color:red&#34;&gt;5&lt;/span&gt;: input data point
Point &lt;span style=&#34;color:red&#34;&gt;6&lt;/span&gt;: input data point perturbed by an $l_1$ constrained perturbation
Point &lt;span style=&#34;color:red&#34;&gt;7&lt;/span&gt;: input data point perturbed by an $l_2$ constrained perturbation&lt;/p&gt;
&lt;p&gt;We denote random unconstrained perturbation by point 1. From our knowledge of $l_p$ norms and pseudo code shown in Fig. 8, we know that these perturbations can be constrained by $l_1$ or $l_2$ norms i.e. within $l_1$ sphere (pink sphere around origin) or $l_2$ spheres(green pyramid structure around origin) respectively as shown in Fig. 9. These sphere represent the space (intutively) where we constraint perturbations we want to add to the original input data (images). Hence, in Viewmaker Network, when perturbation is constrained by an $l_1$ norm, intuitivetly, it implies that perturbation can not lie beyond the space of pink sphere.&lt;/p&gt;
&lt;p&gt;From the psuedo code Step 3, we also see that perturbations are added to original images which is eventually used an as input to the encoder, i.e.&lt;/p&gt;
&lt;p&gt;$$
X = X + P
$$&lt;/p&gt;
&lt;p&gt;where $X$ is the input original data and $P$ is the pertubation weighted by $l_1$ or $l_2$ norm. In Fig. 9, this is computed using simple vector additions. Hence, $P_5$ + $P_3$ = $P_6$ and $P_5$ + $P_2$ = $P_7$ where&lt;br&gt;
$P_5$ is the original input data, $P_3$ and $P_2$ are $P_1$ (perturbation) weighted by $l_1$ and $l_2$ norm respectively, $P_6$ and $P_7$ are the resultant adverserially perturbed representation of original data $P_5$ constrained by $l_p$ norm.&lt;/p&gt;
&lt;p&gt;So far we covered perturbations contstrained by $l_p$ norms but how do we know they would be useful. To ensure that perturbations are useful, there are three things to keep in mind:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;b&gt;Challenging&lt;/b&gt;: The perturations should be complex and strong enough so that the encoder learns useful representations.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Faithful&lt;/b&gt;: Perturbations shouldn’t make the encoder task impossible by destroying all features.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Stochastic&lt;/b&gt;: The method should generate a variety of perturbations Achieved by injecting random noise into viewmaker (so that the model learns a stochastic function that produces a different perturbation each time)&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;52-visual-representation-of-psuedo-code&#34;&gt;5.2. Visual Representation of Psuedo Code&lt;/h2&gt;
&lt;p&gt;Figure 9 shows how respective steps of the psuedo code allign with the block diagram of viewmaker network as whole. First an input data is sampled and a random noise/perturbation is generated. This random perturbation is then weighted by an $l_p$ norm to constraint the perturbations. It is important to constraint these perturbations so that they do not destroy the input data when added to original image. Finally this output is clamped so that each pixel value has a value between 0 and 1 for image representation.&lt;/p&gt;
&lt;figure&gt;
&lt;img src=&#34;https://i.imgur.com/nnN6Qzp.png&#34; alt=&#34;Trulli&#34; /&gt;
&lt;figcaption style=&#34;text-align:center&#34;&gt;
    Fig.9.Psuedo Code: Generate perturbations
&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;!-- Should we conclude it? --&gt;
&lt;!-- conclude with the papers which ppularized adverserial learning --&gt;
&lt;!-- Important paper: Adversarial Self-Supervised Contrastive Learning --&gt;
&lt;h1 id=&#34;7-intro-to-contrastive-learning&#34;&gt;7. Intro to Contrastive Learning&lt;/h1&gt;
&lt;p&gt;Contrastive self-supervised visual representation learning algorithms learns representations by maximizing the agreement between the different views of the same image and minimizing the agreements between views of different images via a contrastive loss in the latent space. SimCLR and InstDisc are two popular contrastive learning algorithms.&lt;strong&gt;(give references)&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;One of the major components of the contrastive learning frameworks is the  data augmentation module which generates the various views of images. Traditionally, this module generates views from a predefined pool of augmentation which are designed by humans. Viewmaker Networks([1]) proposes an alternate data augmentation strategy which can be used for contrastive learning algorithms.&lt;/p&gt;
&lt;center&gt;
&lt;figure&gt;
&lt;img align=&#34;centre&#34; src=&#34;https://i.imgur.com/iCJLj6l.gif&#34; alt=&#34;Trulli&#34; style=&#34;width:60%&#34;&gt;
&lt;figcaption style=&#34;text-align:center&#34;&gt;
    Fig.14: Contrastive Learning(representational image)
&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;/center&gt;
&lt;h1 id=&#34;8-viewmaker-architecture&#34;&gt;8. Viewmaker Architecture&lt;/h1&gt;
&lt;p&gt;The Viewmaker training setup requires the use of two neural networks. The first network is the Encoder network, which is the network that learns the image representations and solves the self-supervised task. For the experiments within this work, ResNet18 is used as the default architecture for the encoder. The Viewmaker network architecture modifies input images before sending them to the Encoder. The Viewmaker architecture was based off of a style transfer network established in previous works in the style transfer literature. The network is fully convolutional, contains 5 residual blocks, and includes no pooling. Uniform random noise is concatenated to the input and activations before each residual block.&lt;/p&gt;
&lt;p&gt;During training, the Encoder and Viewmaker network are trained back-and-forth, similar to other adversarial approaches. The Viewmaker network requires the experimenter to select a self-supervised learning strategy, which can be arbitrarily selected from a variety of available self-supervised methods. The authors opt to use the SimCLR training object and the InstDisc training object. The following hyperparams were selected for each of the training objectives:&lt;/p&gt;
&lt;details&gt;
  &lt;summary&gt;The following hyperparams were selected for each of the training objectives (toggle to see):&lt;/summary&gt;
    &lt;ol&gt;
    &lt;li&gt;SimCLR Specific Hyperparams&lt;/li&gt;
        &lt;ul&gt;
            &lt;li&gt;temperature = 0.07&lt;/li&gt;
        &lt;/ul&gt;
    &lt;li&gt;InstDisk&lt;/li&gt;
        &lt;ul&gt;
            &lt;li&gt;negatives = 4096&lt;/li&gt;
            &lt;li&gt;update rate = 0.5&lt;/li&gt;
        &lt;/ul&gt;
    &lt;li&gt;Other Hyperparams:&lt;/li&gt;
        &lt;ul&gt;
        &lt;li&gt;optimizer = SGD&lt;/li&gt;
        &lt;li&gt;Batch size = 256&lt;/li&gt;
        &lt;li&gt;Learning rate = 0.03&lt;/li&gt;
        &lt;li&gt;Momentum = 0.9&lt;/li&gt;
        &lt;li&gt;Weight decay = 1e-4&lt;/li&gt;
        &lt;li&gt;Epochs = 200&lt;/li&gt;
    &lt;/ol&gt;
&lt;/details&gt;
&lt;h3 id=&#34;evaluation&#34;&gt;Evaluation:&lt;/h3&gt;
&lt;p&gt;To evaluate the resultant encoder models, the authors use linear evaluation. Linear evaluation involves first removing the final layer of the encoder and freezing the remaining paramaters, and then training a logistic regression model using the frozen features. The goal of this strategy is to determine if the learned representations are linearly separable with respect to the true underlying classes, despite the fact that the encoder is not trained using those class label. The experimental setup for training the logistic regression model is selected as follows:&lt;/p&gt;
&lt;details&gt;
  &lt;summary&gt;The following hyperparams were selected for each of the training objectives (toggle to see):&lt;/summary&gt;
    &lt;ol&gt;
    &lt;li&gt;Optimizer = SGD&lt;/li&gt;
    &lt;li&gt;Learning rate = 0.01&lt;/li&gt;
    &lt;li&gt;Momentum = 0.9&lt;/li&gt;
    &lt;li&gt;Weight decay = 0&lt;/li&gt;
    &lt;li&gt;Batch size = 128&lt;/li&gt;
    &lt;li&gt;Epochs = 100&lt;/li&gt;
    &lt;li&gt;Learning rate decay schedule:&lt;/li&gt;
        &lt;ul&gt;
        &lt;li&gt;epoch 60, decay by factor of 10&lt;/li&gt;
        &lt;li&gt;epoch 80, decay by factor of 10&lt;/li&gt;
    &lt;/ol&gt;
&lt;/details&gt;
&lt;h3 id=&#34;comparitive-results&#34;&gt;Comparitive results&lt;/h3&gt;
&lt;p&gt;They compare their results with SimCLR (reference) and IntDisc (reference) training object, which uses expertly selected image augmentations.&lt;/p&gt;
&lt;center&gt;
&lt;figure&gt;
&lt;img align=&#34;centre&#34; src=&#34;https://i.imgur.com/TTzDVN6.png&#34; alt=&#34;Trulli&#34; style=&#34;width:60%&#34;&gt;
&lt;figcaption style=&#34;text-align:center&#34;&gt;
    Fig.10:  SimCLR expert views transfer performance as compared to viewmaker learned views on CIFAR-10
&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;/center&gt;
&lt;hr&gt;
&lt;h1 id=&#34;8-experiments&#34;&gt;8. Experiments&lt;/h1&gt;
&lt;h2 id=&#34;81-collapse-problem-in-self-supervised-learning-and&#34;&gt;8.1 Collapse Problem in Self-Supervised Learning and&lt;/h2&gt;
&lt;h4 id=&#34;introduction&#34;&gt;Introduction&lt;/h4&gt;
&lt;p&gt;Since the beginning of the field of self-supervised visual representation learning, researchers have proposed various methods to overcome the problem of representational collapse (also known as complete collapse) i.e. the problem where all embedding vectors collapse to a trivial constant vector. Contrastive learning methods like SimCLR solve this problem by incorporating negative pairs of images.&lt;/p&gt;
&lt;p&gt;Although complete collapse is avoided in contrastive learning methods (via the use of positive and negative pairs), a different form of collapse occurs and is often overlooked: dimensional collapse([3]). In dimensional collapse the embedding vectors rather than occupying the entire available embedding space, occupies only a lower-dimensional manifold within the available embedding space.&lt;/p&gt;
&lt;figure&gt;
&lt;img src=&#34;https://i.imgur.com/jaFWhxU.png&#34; alt=&#34;Trulli&#34; /&gt;
&lt;figcaption style=&#34;text-align:center&#34;&gt;
    Fig.9 (a) Complete collapse and (b) Dimensional collapse
&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;h4 id=&#34;implementation-and-experiments&#34;&gt;Implementation and Experiments&lt;/h4&gt;
&lt;p&gt;To observe and compare the dimensional collapse occuring in the SimCLR pre-training setup while using expert views and viewmaker views, we followed the procedure proposed in [3].&lt;/p&gt;
&lt;p&gt;We trained two ResNet-18 models, one with expert views and the other with viewmaker views, on CIFAR-10, following the standard SimCLR training procedure given in [1]. After training, we passed the test set through both of the pretrained ResNet-18 models and obtained 2 sets of embedding vectors.&lt;/p&gt;
&lt;p&gt;Let us denote the two embedding matrices as:
$\mathbf{Z}_e$ : Embedding matrix obtained from expert views pretrained ResNet-18
$\mathbf{Z}_v$ : Embedding matrix obtained from viewmaker views pretrained ResNet-18&lt;/p&gt;
&lt;p&gt;$$\mathbf{Z}_e , \mathbf{Z}_v \in \mathbb{R} ^{10000 \times 128}$$
[10000 is the number of images in the test set of CIFAR-10 and 128 is the output embedding dimension of ResNet-18]&lt;/p&gt;
&lt;p&gt;We then compute the covariance matrix of the embedding layers of both the networks using the following formula:&lt;/p&gt;
&lt;p&gt;$$\mathbf{C} = \frac{1}{N} \sum_{i=1}^{N} (\mathbf{z}_i - \mathbf{\bar z})^T(\mathbf{z}_i - \mathbf{\bar z})$$&lt;/p&gt;
&lt;p&gt;where,&lt;/p&gt;
&lt;p&gt;$\mathbf{C}$ : Covariance matrix
$N$ : Number of images in the test set of CIFAR-10(10000 images)
$\mathbf{z}_i$ : $i^{th}$ row vector of $\mathbf{Z}$ ($\mathbf{Z}$ $\in$ {$\mathbf{Z}_e$, $\mathbf{Z}&lt;em&gt;v$ })
and $\mathbf{\bar z} = \frac{1}{N} \sum&lt;/em&gt;{i=1}^{N} \mathbf{z}_i$&lt;/p&gt;
&lt;p&gt;Thus, we obtain:
$\mathbf{C}_e$ : Covariance matrix obtained from the embedding layer of expert views pretrained ResNet-18
and
$\mathbf{C}_v$ : Covariance matrix obtained from the embedding layer of viewmaker views pretrained ResNet-18
$$\mathbf{C}_e, \mathbf{C}_v \in \mathbb{R} ^{128 \times 128}$$&lt;/p&gt;
&lt;p&gt;We then perform the SVD(Singular Value Decomposition) on both $\mathbf{C}_e$ and $\mathbf{C}_v$
$$\mathbf{C} = \mathbf{U} \mathbf{S} \mathbf{V}^T,  \mathbf{S} = diag(\sigma^k)$$&lt;/p&gt;
&lt;p&gt;We can then plot the singular values thus obtained from both $\mathbf{C}_e$ and $\mathbf{C}_v$ in sorted order(descending) and logarithmic scale to obtain the following plot:&lt;/p&gt;
&lt;center&gt;
&lt;figure&gt;
&lt;img align=&#34;centre&#34; src=&#34;https://i.imgur.com/Lg7kci2.png&#34; alt=&#34;Trulli&#34;&gt;
&lt;figcaption style=&#34;text-align:center&#34;&gt;
    Fig.17: Singular value spectrum of the embedding spaces 
&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;/center&gt;
&lt;h4 id=&#34;observations&#34;&gt;Observations&lt;/h4&gt;
&lt;p&gt;The figure shows that a large number of the singular values of both covariance matrices are collapsing to zero, which represents the collapsing dimensions in the respective embedding spaces. $\mathbf{S}_e$ collapses to 24 dimensions(out of the available 128) and $\mathbf{S}_v$ collapses to 28 dimensions(out of the available 128).&lt;/p&gt;
&lt;p&gt;[3] shows that a mechanism which causes dimensional collapse is strong augmentation. Although both methods results in dimensional collapse, Viewmaker views pretrained ResNet-18 had marginally less number of embedding dimensions that collapsed. We believe this can be attributed to the constrained perturbations resulting in comparably less strong augmentations.&lt;/p&gt;
&lt;p&gt;It is known that neural networks that can generalize better to the test set are those that transform the data into low-dimensional manifolds. There is a proven tradeoff between generalization and class separability of representations w.r.t the dimensionality of the representations. We believe that marginally lesser number of collapsed dimensions of the Viewmaker views pretrained ResNet-18&amp;rsquo;s embedding layer helps improving the class separability of the representations but is hurting it&amp;rsquo;s generalization capability due to the above mentioned tradeoff, which is evident from the marginally lower accuracy of Viewmaker views pretrained ResNet-18 on transfer tasks &lt;strong&gt;(see section on viewmaker architecture)&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id=&#34;82-do-expert-views-and-viewmaker-trained-networks-learn-similar-representations&#34;&gt;8.2 Do Expert Views and Viewmaker Trained Networks Learn Similar Representations?&lt;/h2&gt;
&lt;h4 id=&#34;introduction-1&#34;&gt;Introduction&lt;/h4&gt;
&lt;p&gt;Contrastive self-supervised methods, such as those trained using &amp;ldquo;Expert Views&amp;rdquo;, exploit the fact that image semantics are invariant under a variety of image transformations. Given a set of transformations which are known to preserve image semantics, networks trained using this method can develop the weak underlying signal within a dataset and learn many relevant features for solving downstream image tasks. Networks trained with &amp;ldquo;Expert Views&amp;rdquo; typically use the following set of image transformations:&lt;/p&gt;
&lt;p&gt;A = {Crop, Cutout, Noise, Rotation, Flipping, Resizing, Color Distortion, and Sobel Filtering}&lt;/p&gt;
&lt;p&gt;In contrast to this, the Viewmaker method attempts to learn these image transformations. This creates an interesting situation, as both methods are attempting to learn features corresponding to image properties which are unchanged under various transformations. However, these methods rely on potentially disjoint sets of transformations. This situation naturally leads to the following question &amp;ndash; do these views lead to models that learn similar representations of the data?&lt;/p&gt;
&lt;p&gt;Developing similarity metrics for determining the similarity between models is a challenging problem. Recently, Centered Kernel Alignment (CKA) [^4] has emerged as a popular strategy for measuring the similarity between network architectures in the representation space. We use this metric to explore the question of whether or not Viewmaker and Expert Views trained models learn similar representations. Below, we develop the mathematical intuition behind this metric and then highlight a set of experiments which we conducted to answer our question.&lt;/p&gt;
&lt;h4 id=&#34;mathematics&#34;&gt;Mathematics&lt;/h4&gt;
&lt;p&gt;CKA measures the similarity between two representations, $\mathbf{X} \in \mathbb{R}^{m\times p_1}$ and $\mathbf{Y} \in \mathbb{R}^{m\times p_2}$, where $m$ is number of examples, $p_1$ is the dimensionality of the representations in $\mathbf{X}$ and $p_2$ is the dimensionality of the representations in $\mathbf{Y}$. This metric is bound between 0 and 1, with 0 representing maximum dissimilarity and 1 representing maximum similarity. CKA satisfies two important propertis:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Invariant to Orthogonal Transformations&lt;/li&gt;
&lt;li&gt;Invariance to Isotropic Scaling: i.e., $s(\mathbf{X}, \mathbf{Y}) = s(\alpha\mathbf{X}, \beta\mathbf{Y})$&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;CKA itself is dependent on the Hilbert-Schmidt Independence Criterion (HSIC), which only satisfies first property. HSIC is calculated as follows:&lt;/p&gt;
&lt;p&gt;$$\texttt{HSIC}(\mathbf{K},\mathbf{L}) = \frac{1}{(n-1)^2}\texttt{tr}(\mathbf{K}\mathbf{H}\mathbf{L}\mathbf{H})$$&lt;/p&gt;
&lt;p&gt;Where $\mathbf{K}_{ij} = k(\mathbf{X}_i, \mathbf{X}&lt;em&gt;j) \in \mathbb{R}^{m \times m}$ and $\mathbf{L}&lt;/em&gt;{ij} = k(\mathbf{Y}_i, \mathbf{X}_j) \in \mathbb{R}^{m \times m}$ are a Gram matrices generated by a kernel function $k$, and $\mathbf{H} = \mathbf{I}_m - \frac{1}{m}\mathbf{J}_m$ is a kernel centering matrix. HSIC can be made invariant to isotropic scaling by normalizing the metric, which leads to the following definition for CKA:&lt;/p&gt;
&lt;p&gt;$$\texttt{CKA}(\mathbf{K},\mathbf{L}) = \frac{\texttt{HSIC}(\mathbf{K},\mathbf{L})}{\sqrt{\texttt{HSIC}(\mathbf{K},\mathbf{K})\texttt{HSIC}(\mathbf{L},\mathbf{L})}}$$&lt;/p&gt;
&lt;p&gt;For our experiments, we select the Radial Basis Function for our kernel function $k$, which is defined as:
$$k(\mathbf{X}_i, \mathbf{X}_j) = \texttt{exp}(-\frac{||\mathbf{X}_i - \mathbf{X}_j ||^2}{2\sigma^2})$$&lt;/p&gt;
&lt;h3 id=&#34;experiments&#34;&gt;Experiments&lt;/h3&gt;
&lt;center&gt;
&lt;figure&gt;
&lt;img align=&#34;centre&#34; src=&#34;https://i.imgur.com/QdMNsBJ.png&#34; alt=&#34;Trulli&#34; style=&#34;width:100%&#34;&gt;
&lt;figcaption style=&#34;text-align:center&#34;&gt;
    Fig.17: (Adrian please give a caption here)
&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;/center&gt;
&lt;p&gt;To determine the relationship between the functions learned by Viewmaker and Expert Views, we calculate the CKA between 6 layers at increasing network depths across both models and within each model. Figure 17 highlights the three CKA heatmaps for the following comparisons:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Viewmaker layers vs. Viewmaker layers&lt;/li&gt;
&lt;li&gt;Expert View layers vs. Expert View layers&lt;/li&gt;
&lt;li&gt;Viewmaker layers vs. Expert View layers&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;For every layer considered, we calculate the CKA with respect to every other layer (including the layer being considered). In the CKA heatmaps shown above, the shallowest layers are represented in the bottom left corner of the plot, and the deepest layers are represented in the top right corner of the plot.&lt;/p&gt;
&lt;h3 id=&#34;observations-1&#34;&gt;Observations&lt;/h3&gt;
&lt;p&gt;A few interesting relationships can be observed from these plots. First, we find that Viewmaker and Expert Views learn similar features across the shallower layers but learn consideribly different features as the depth of the network increases. Second, the shallowest layers of Expert Views have a relatively high similarity with many of the deeper layers of Viewmaker. These two points suggest that Viewmaker networks learn a different function of the data when compared to Expert Views, which should be carefully considered when deciding to train with Viewmaker view over Expert views.&lt;/p&gt;
&lt;h1 id=&#34;9-conclusion&#34;&gt;9. Conclusion&lt;/h1&gt;
&lt;p&gt;In this blog post we explore Viewmaker Networks, a strategy for learning image transformations and which can be used by non-domain experts. We make observations about the learned representations, exploring whether there dimensionality collapse inn the embedding space and how they compare to other strategies for selecting views.&lt;/p&gt;
&lt;hr&gt;
&lt;details&gt;
  &lt;summary&gt;Image Credits (toggle to see):&lt;/summary&gt;
    &lt;ol&gt;
    &lt;li&gt;[Fig.13](https://arxiv.org/abs/2110.09348)&lt;/li&gt;
    &lt;/ol&gt;
&lt;/details&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;[1] 
&lt;a href=&#34;&#34;&gt;ImageNet Classification with Deep Convolutional Neural Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;[2] 
&lt;a href=&#34;&#34;&gt;Random Erasing Data Augmentation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;[3] 
&lt;a href=&#34;&#34;&gt;Improved Regularization of Convolutional Neural Networks with Cutout&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;[4] 
&lt;a href=&#34;&#34;&gt;mixup: Beyond Empirical Risk Minimization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;[5] 
&lt;a href=&#34;&#34;&gt;CutMix: Regularization Strategy to Train Strong Classifiers with Localizable Features&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;[6] 
&lt;a href=&#34;&#34;&gt;Data Augmentation Generative Adversarial Network&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;[7] 
&lt;a href=&#34;&#34;&gt;Improved Regularization of Convolutional Neural Networks with Cutout) and mixup (mixup: Beyond Empirical Risk Minimization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;[8] 
&lt;a href=&#34;&#34;&gt;mixup: Beyond Empirical Risk Minimization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;[9] 
&lt;a href=&#34;&#34;&gt;Improving Robustness Without Sacrificing Accuracy with Patch Gaussian Augmentation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;[10] 
&lt;a href=&#34;&#34;&gt;Representation Learning with Contrastive Predictive Coding&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;[11] 
&lt;a href=&#34;&#34;&gt;On the Suitability of Lp-norms for Creating and Preventing Adversarial Examples&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;[12] 
&lt;a href=&#34;https://arxiv.org/abs/2002.05709&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;A Simple Framework for Contrastive Learning of Visual Representations&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;[13] 
&lt;a href=&#34;&#34;&gt;Unsupervised Feature Learning via Non-Parametric Instance-level Discrimination&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;[14] 
&lt;a href=&#34;&#34;&gt;Viewmaker Network&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;[15] 
&lt;a href=&#34;https://arxiv.org/abs/2110.09348&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Understanding Dimensional Collapse in Contrastive Self-supervised Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;[16] 
&lt;a href=&#34;https://arxiv.org/abs/1905.00414&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Similarity of Neural Network Representations Revisited&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;p&gt;&lt;em&gt;Acknowledgment&lt;/em&gt;: This work was done with Adrian and Vaisakh as part of our collaboration at Machine Learning Collective. Hude thanks to Akshit for allowing me to use his GPU allowing me to run these experiments.&lt;/p&gt;
&lt;!-- ---

# 9. Conclusion
Putting it all together, future of conditional view generation.  --&gt;
&lt;!-- 
&lt;details&gt;
  &lt;summary&gt;Image Credits (toggle to see):&lt;/summary&gt;
    &lt;ol&gt;
    &lt;li&gt;[Fig.9](https://arxiv.org/abs/2110.09348)&lt;/li&gt;
    &lt;li&gt;[Fig.9](https://arxiv.org/abs/2110.09348)&lt;/li&gt;
    &lt;/ol&gt;
&lt;/details&gt; --&gt;
&lt;!-- ## **References**

Write the main references on the top.

[1] [Viewmaker Networks: Learning Views for Unsupervised Representation Learning](https://iclr.cc/virtual/2021/poster/2544)
[2] [A Simple Framework for Contrastive Learning of Visual Representations](https://arxiv.org/abs/2002.05709)
[3] [Understanding Dimensional Collapse in Contrastive Self-supervised Learning](https://arxiv.org/abs/2110.09348)
[4] [Similarity of Neural Network Representations Revisited](https://arxiv.org/abs/1905.00414) --&gt;
</description>
    </item>
    
    <item>
      <title>Understanding Usage of plt, figure, subplot, axes, axis in matplotlib</title>
      <link>https://srishti.dev/post/1111-11-11-plt-fig-axis/</link>
      <pubDate>Sat, 11 Nov 1111 12:38:01 -0700</pubDate>
      <guid>https://srishti.dev/post/1111-11-11-plt-fig-axis/</guid>
      <description>&lt;p&gt;When working with python libraries, especially for visualization, I usually get confused my number of options available for plotting. Example:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;1. plt.plot()

2. ax = plt.subplot()
   ax.plot(x, y)

3. fig1, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2)

4. f, axarr = plt.subplots(2,2)
   axarr[0,0].imshow(image_datas[0])
   ....

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;After reading through a bunch of stackoverflow explainations, I compiled them here:&lt;/p&gt;
&lt;br/&gt;
&lt;h3 id=&#34;question-1-what-is-the-difference-between-drawing-plots-using-plot-axes-or-figure-in-matplotlib&#34;&gt;Question 1: What is the difference between drawing plots using plot, axes or figure in matplotlib?&lt;/h3&gt;
&lt;br/&gt;
&lt;p&gt;&lt;strong&gt;Plot just one figure with (x,y) coordinates&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;plt.plot(x, y)

&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If you just want to get one graphic, you can use this way.&lt;/p&gt;
&lt;p&gt;Example:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;import numpy as np
import matplotlib.pyplot as plt

x = np.random.rand(10)
y = np.random.rand(10)

figure1 = plt.plot(x,y)
&lt;/code&gt;&lt;/pre&gt;
&lt;figure&gt;
    &lt;img src=&#34;https://srishti.dev/img/plt-img-1.png&#34; width=&#34;300&#34; height=&#34;400&#34; /&gt;
    &lt;figcaption&gt;
    &lt;a href=&#34;https://stackoverflow.com/questions/37970424/what-is-the-difference-between-drawing-plots-using-plot-axes-or-figure-in-matpl&#34;&gt; Fig 1. Photo via Stackoverflow&lt;/a&gt;
    &lt;/figcaption&gt; 
 &lt;/figure&gt;
&lt;p&gt;&lt;strong&gt;Plot one or several figure(s) in the same window&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Plot just one figure&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;ax = plt.subplot()
ax.plot(x, y)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;or you can plot multiple figures like this:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;fig1, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This plot 4 figures which are named ax1, ax2, ax3 and ax4 each one but on the same window. This window will be just divided in 4 parts with my example.&lt;/p&gt;
&lt;p&gt;Example:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;import numpy as np
import matplotlib.pyplot as plt

x1 = np.random.rand(10)
x2 = np.random.rand(10)
x3 = np.random.rand(10)
x4 = np.random.rand(10)
y1 = np.random.rand(10)
y2 = np.random.rand(10)
y3 = np.random.rand(10)
y4 = np.random.rand(10)

figure2, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2)
ax1.plot(x1,y1)
ax2.plot(x2,y2)
ax3.plot(x3,y3)
ax4.plot(x4,y4)

plt.show()
&lt;/code&gt;&lt;/pre&gt;
&lt;figure&gt;
    &lt;img src=&#34;https://srishti.dev/img/plt_img_2.png&#34; width=&#34;300&#34; height=&#34;400&#34; /&gt;
    &lt;figcaption&gt;
    &lt;a href=&#34;https://stackoverflow.com/questions/37970424/what-is-the-difference-between-drawing-plots-using-plot-axes-or-figure-in-matpl&#34;&gt; Fig 2. Photo via Stackoverflow&lt;/a&gt;
    &lt;/figcaption&gt; 
 &lt;/figure&gt;
&lt;p&gt;&lt;strong&gt;Another method for multiple plots&lt;/strong&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;
import numpy as np
import matplotlib.pyplot as plt

fig = plt.figure()
new_plot = fig.add_subplot(111)
new_plot.plot(x, y)
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;br/&gt;
&lt;h3 id=&#34;question-2-difference-between-axes-and-axis-in-matplotlib&#34;&gt;Question 2: Difference between “axes” and “axis” in matplotlib?&lt;/h3&gt;
&lt;p&gt;In the context of matplotlib, axes is not the plural form of axis, it actually denotes the plotting area, including all axis.&lt;/p&gt;
&lt;figure&gt;
    &lt;img src=&#34;https://srishti.dev/img/axes_axis.png&#34; width=&#34;300&#34; height=&#34;400&#34; /&gt;
    &lt;figcaption&gt;
    &lt;a href=&#34;https://matplotlib.org/1.5.1/faq/usage_faq.html#parts-of-a-figure&#34;&gt; Fig 3. Photo via matplotlib old documentation&lt;/a&gt;
    &lt;/figcaption&gt; 
 &lt;/figure&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Axes&lt;/p&gt;
&lt;p&gt;This is what you think of as &lt;code&gt;a plot&lt;/code&gt;, it is the region of the image with the data space (marked as the inner blue box). A given figure can contain many Axes, but a given Axes object can only be in one Figure. The Axes contains two (or three in the case of 3D) Axis objects (be aware of the difference between Axes and Axis) which take care of the data limits (the data limits can also be controlled via set via the &lt;code&gt;set_xlim()&lt;/code&gt; and &lt;code&gt;set_ylim()&lt;/code&gt; Axes methods). Each Axes has a title (set via &lt;code&gt;set_title()&lt;/code&gt;), an x-label (set via &lt;code&gt;set_xlabel()&lt;/code&gt;), and a y-label set via &lt;code&gt;set_ylabel()&lt;/code&gt;).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Axis&lt;/p&gt;
&lt;p&gt;These are the number-line-like objects (circled in green). They take care of setting the graph limits and generating the ticks (the marks on the axis) and ticklabels (strings labeling the ticks). The location of the ticks is determined by a Locator object and the ticklabel strings are formatted by a Formatter. The combination of the correct Locator and Formatter gives very fine control over the tick locations and labels.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;br/&gt;
&lt;h3 id=&#34;question-3-what-is-the-difference-between-pltsubplots-and-pltfigure&#34;&gt;Question 3: What is the difference between plt.subplots() and plt.figure()&lt;/h3&gt;
&lt;p&gt;In matplotlib, we can plots in two ways like below:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;plt.figure(1,figsize=(400,8))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;or&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;fig,ax = plt.subplots()
fig.set_size_inches(400,8)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and though both are correct, they have their differences.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;plt.figure&lt;/code&gt; just creates a figure (but with no axes in it) whereas &lt;code&gt;plt.subplots&lt;/code&gt; takes optional arguments (ex: plt.subplots(2, 2)) to create an array of axes in the figure. Most of the kwargs that &lt;code&gt;plt.figure&lt;/code&gt; takes &lt;code&gt;plt.subplots&lt;/code&gt; also takes.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;plt.figure()&lt;/code&gt; is usually used when you want more customization to you axes, such as positions, sizes, colors and etc. You can see artist tutorial for more details. (I personally prefer this for individual plot).&lt;/p&gt;
&lt;p&gt;&lt;code&gt;plt.subplots()&lt;/code&gt; is recommended for generating multiple subplots in grids. You can also achieve higher flexibility using &amp;lsquo;gridspec&amp;rsquo; and &amp;lsquo;subplots&amp;rsquo;, see details here.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>CLIP: Connecting Text and Images</title>
      <link>https://srishti.dev/post/1111-11-11-clip-connecting-text-and-images/</link>
      <pubDate>Sat, 11 Nov 1111 12:38:00 -0700</pubDate>
      <guid>https://srishti.dev/post/1111-11-11-clip-connecting-text-and-images/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;https://srishti.dev/img/clip-openai.png&#34; width=&#34;920&#34; height=&#34;720&#34; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Paper&lt;/strong&gt;: Learning Transferable Visual Models From Natural Language Supervision&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;PDF: 
&lt;a href=&#34;https://cdn.openai.com/papers/Learning_Transferable_Visual_Models_From_Natural_Language_Supervision.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Learning Transferable Visual Models From Natural Language Supervision&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Blog: 
&lt;a href=&#34;https://openai.com/blog/clip/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;CLIP: Connecting Text and Images&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;General Terms:&lt;/strong&gt; &lt;/b&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;u&gt;Contrastive Learning&lt;/u&gt;: It is based on the intuition that you can contrast/differentiate between similar and dissimilar things. In machine learning model, we formulate this as a task of finding similar and dissimilar things i.e. the model should be able to classify between similar and dissimilar images&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;u&gt;Zero-shot Learning &lt;/u&gt;: Zero-shot learning is a problem, where at test stage,the model/learner aims to recognize objects whose instances maynot have been seen during training. To learn about various works that have been done in this space (atleast till 2018), this paper provides good details: 
&lt;a href=&#34;https://arxiv.org/pdf/1707.00600.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Zero-shot learning—a comprehensive evaluation of the good, the bad and the ugly&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;hr&gt;
&lt;p&gt;The paper describes an approach where it takes a large dataset of image text pair and tries to learn a model that scores whether a image and text could co-occur. This is learned over a large dataset.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Question: How to do classification zero shot i.e. without any training?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Given a specified classification task where there are some images and some labels and you are supposed to evaluate based on our prediction, you will embed all text labels into vectors and images into vectors and them compare the score of their cross product. The pairing that is going to give the highest score is going to be the prediction label.&lt;/p&gt;
 &lt;figure&gt;
    &lt;img src=&#34;https://srishti.dev/img/clip-architecture.png&#34; width=&#34;920&#34; height=&#34;720&#34;/&gt;
    &lt;figcaption&gt;
    &lt;a href=&#34;https://cdn.openai.com/papers/Learning_Transferable_Visual_Models_From_Natural_Language_Supervision.pdf&#34;&gt; Fig 1. Photo via Open AI Paper&lt;/a&gt;
    &lt;/figcaption&gt; 
 &lt;/figure&gt;
&lt;p&gt;In Fig.1, $N$ is the size of images associated with some text. $T_i$ is the encoding for the entire text string.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Why is it zero shot?&lt;/strong&gt;
This model is zero shot because it refers to the number of lables we see for training. In this architecture, for a particular evaluation task, you do not train. You take the train weights(and give some bias). THis notion of zero shot is in line with GPT notion of zero shot i.e. you do a lot of training -  you don&amp;rsquo;t care what is happenng at the pre-training stage&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Dataset:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Called WiT Dataset (WebImageText)&lt;/li&gt;
&lt;li&gt;400 million (image, text) pairs by searching over text queries&lt;/li&gt;
&lt;li&gt;500,000 text queries:
&lt;ul&gt;
&lt;li&gt;All words occuring atleast 100 times in the English version of Wikipedia + WordNet synsets + some details&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Cap at a maximum of 20000 (image, text) pairs per query&lt;/li&gt;
&lt;li&gt;Total wordcount similar to WebText dataset used by GPT-2&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;u&gt;Note&lt;/u&gt;: There is a difference between queries and labels.
Example, when you give a query &amp;lsquo;dog&amp;rsquo; on google image, it will come with all sorts of images of &amp;lsquo;dog&amp;rsquo; and each such image will come associated with a (paired)text. These texts can be in the form of alt text or title of the page.&lt;/p&gt;
 &lt;figure&gt;
    &lt;img src=&#34;https://srishti.dev/img/clip-pretraining.png&#34; width=&#34;920&#34; height=&#34;720&#34;/&gt;
    &lt;figcaption&gt;
    &lt;a href=&#34;https://cdn.openai.com/papers/Learning_Transferable_Visual_Models_From_Natural_Language_Supervision.pdf&#34;&gt; Fig 2. Photo via Open AI Paper&lt;/a&gt;
    &lt;/figcaption&gt; 
 &lt;/figure&gt;
&lt;p&gt;Fig.2 shows various methods authors use to test which pre-training method is better. We know there are various training method that connects image and language. Most vanilla verison is captioning (language modelling with images). In this paper, authors try a variety of pre-training methods. First for captioning - they found it is not very compute efficient. Second, they tried BoW (bag of words) and they find that they scale better. However, the best they found was contrastive learning.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Model architecture:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;u&gt; Image Encode &lt;/u&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ResNet
&lt;ul&gt;
&lt;li&gt;Modifications: anti-aliased max pooling; final global pooling by QK Value (QKV) attention&lt;/li&gt;
&lt;li&gt;Scaling model size by allocating compute equally (to width, depth, input resolution)
OR&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Vision Transformer&lt;/li&gt;
&lt;/ul&gt;
&lt;br/&gt;
&lt;p&gt;&lt;u&gt; Text Encoder &lt;/u&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Transformer text encoder
&lt;ul&gt;
&lt;li&gt;scaling model size by width only; do not scale depth&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;br/&gt;
&lt;u&gt; Other parameters &lt;/u&gt;
&lt;ul&gt;
&lt;li&gt;$N$ x $N$ affinity matrix. Symmetric Contrastic Loss&lt;/li&gt;
&lt;li&gt;Temperature $t$ is initialized to 0.07 but allowed to learn&lt;/li&gt;
&lt;li&gt;Adam with decoupled weight decay ($adam_w$). Cosine LR.&lt;/li&gt;
&lt;li&gt;Batch size 32768 (this is huge!)&lt;/li&gt;
&lt;/ul&gt;
&lt;!DOCTYPE html&gt;
&lt;html&gt;
&lt;head&gt;
&lt;style&gt;
* {
  box-sizing: border-box;
}
.column img {
    width: 100%;
    float: left ;
    margin: 0;
}
.column {
    width: 50%;
    float: left ;
    padding: 10px; 
}
.row::after {
  content: &#34;&#34;;
  clear: both;
  display: table;
}
&lt;/style&gt;
&lt;/head&gt;
&lt;body&gt;
&lt;div class=&#34;row&#34;&gt;
    &lt;div class=&#34;column&#34;&gt;
        &lt;img src=&#34;https://srishti.dev/img/clip-prompt-engineering-2.png&#34;/&gt;
    &lt;/div&gt;
    &lt;div class=&#34;column&#34;&gt;
        &lt;img src=&#34;https://srishti.dev/img/clip-prompt-engineering.png&#34; /&gt;
    &lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;row&#34;&gt;
    &lt;div class=&#34;column&#34;&gt;
        &lt;figcaption&gt;
            &lt;a href=&#34;https://cdn.openai.com/papers/Learning_Transferable_Visual_Models_From_Natural_Language_Supervision.pdf&#34;&gt; Fig 3a. Photo via Open AI Paper&lt;/a&gt;
        &lt;/figcaption&gt; 
    &lt;/div&gt;
    &lt;div class=&#34;column&#34;&gt;
        &lt;figcaption&gt;
            &lt;a href=&#34;https://cdn.openai.com/papers/Learning_Transferable_Visual_Models_From_Natural_Language_Supervision.pdf&#34;&gt; Fig 3b. Photo via Open AI Paper&lt;/a&gt;
        &lt;/figcaption&gt; 
    &lt;/div&gt;
&lt;/div&gt;
&lt;/body&gt;
&lt;/html&gt;
 &lt;figure&gt;
 &lt;/figure&gt;
&lt;p&gt;&lt;strong&gt;Prompt Engineering:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;From Fig 3.a, we can see that in addition to texts, authors have also experimented using prompts wth texts i.e. you can embed the label itself but you can also embed string with a customized prompt ahead of it.  E.g. &amp;lsquo;a &lt;em&gt;photo&lt;/em&gt; of &amp;lsquo;, &amp;lsquo;a &lt;em&gt;bag&lt;/em&gt; of&amp;rsquo;. Fig 3.b shows that authors show that prompts work better than raw label (+15%)&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Performance:&lt;/strong&gt;&lt;/p&gt;
 &lt;figure&gt;
    &lt;img src=&#34;https://srishti.dev/img/clip-zero-shot-performance.png&#34;/&gt;
    &lt;figcaption&gt;
    &lt;a href=&#34;https://cdn.openai.com/papers/Learning_Transferable_Visual_Models_From_Natural_Language_Supervision.pdf&#34;&gt; Fig 4. Photo via Open AI Paper from Appendix Section&lt;/a&gt;
    &lt;/figcaption&gt; 
 &lt;/figure&gt;
&lt;p&gt;For ImageNet (1.2 million), vanilla ResNet50 gives accuracy of 56.3% &lt;cite&gt;[1]&lt;/cite&gt; and zero-shot (CLIP model) gives 59.6%.(see last column of first row in Fig.4). Model (&lt;em&gt;L/14-336x&lt;/em&gt;) that gives classification accuracy close to ResNet-50 uses a lot more compute (see last column, last row in Fig.4). L/14-336x means 14 large with input resolution 336px. Hence, to surpass a supervised ResNet, you need a lot bigger model&lt;/p&gt;
&lt;br/&gt;
&lt;p&gt;[1] : &lt;a href=&#34;https://arxiv.org/pdf/1811.06992.pdf&#34;&gt;https://arxiv.org/pdf/1811.06992.pdf&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;u&gt;Acknowledment:&lt;/u&gt; Thanks to the discussion in TTIC reading group which introduced me to this paper.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Datasets for Fine-Grained Image Classification</title>
      <link>https://srishti.dev/post/1111-11-11-datasets-for-fine-grained-image-classification/</link>
      <pubDate>Sat, 11 Nov 1111 12:38:00 -0700</pubDate>
      <guid>https://srishti.dev/post/1111-11-11-datasets-for-fine-grained-image-classification/</guid>
      <description>&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;iNat2017&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;&lt;u&gt;Data:&lt;/u&gt; &lt;a href=&#34;https://github.com/visipedia/inat_comp/tree/master/2017&#34;&gt;&lt;a href=&#34;https://github.com/visipedia/inat_comp/tree/master/2017&#34;&gt;https://github.com/visipedia/inat_comp/tree/master/2017&lt;/a&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;&lt;strong&gt;iNat2018 and iNat2019&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;&lt;u&gt;Data:&lt;/u&gt; &lt;a href=&#34;https://github.com/visipedia/inat_comp/blob/master/2018/README.md&#34;&gt;&lt;a href=&#34;https://github.com/visipedia/inat_comp/blob/master/2018/README.md&#34;&gt;https://github.com/visipedia/inat_comp/blob/master/2018/README.md&lt;/a&gt; &lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;u&gt;Data:&lt;/u&gt; &lt;a href=&#34;https://github.com/visipedia/inat_comp&#34;&gt;&lt;a href=&#34;https://github.com/visipedia/inat_comp&#34;&gt;https://github.com/visipedia/inat_comp&lt;/a&gt; &lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;u&gt;Details:&lt;/u&gt; The dataset is similar to iNat2017 with small differences, which are mentioned in the website.&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start=&#34;3&#34;&gt;
&lt;li&gt;&lt;strong&gt;Herbarium Dataset&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;&lt;u&gt;Paper:&lt;/u&gt; &lt;a href=&#34;https://drive.google.com/file/d/1HPyY82IwGkKlp3ow13JCDtn1G-s_mgGJ/view&#34;&gt;&lt;a href=&#34;https://drive.google.com/file/d/1HPyY82IwGkKlp3ow13JCDtn1G-s_mgGJ/view&#34;&gt;https://drive.google.com/file/d/1HPyY82IwGkKlp3ow13JCDtn1G-s_mgGJ/view&lt;/a&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start=&#34;4&#34;&gt;
&lt;li&gt;&lt;strong&gt;Cassava (leaves) images&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;&lt;u&gt;Paper:&lt;/u&gt; &lt;a href=&#34;https://drive.google.com/file/d/1GW0Ak_fS0ZMXcy89B7di1xNF1MBIga_4/view&#34;&gt;&lt;a href=&#34;https://drive.google.com/file/d/1GW0Ak_fS0ZMXcy89B7di1xNF1MBIga_4/view&#34;&gt;https://drive.google.com/file/d/1GW0Ak_fS0ZMXcy89B7di1xNF1MBIga_4/view&lt;/a&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start=&#34;5&#34;&gt;
&lt;li&gt;&lt;strong&gt;Birds&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;&lt;u&gt;Paper:&lt;/u&gt; &lt;a href=&#34;https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Horn_Building_a_Bird_2015_CVPR_paper.pdf&#34;&gt;&lt;a href=&#34;https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Horn_Building_a_Bird_2015_CVPR_paper.pdf&#34;&gt;https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Horn_Building_a_Bird_2015_CVPR_paper.pdf&lt;/a&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start=&#34;6&#34;&gt;
&lt;li&gt;&lt;strong&gt;Animal Species (camera trap)&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;&lt;u&gt;Paper:&lt;/u&gt; &lt;a href=&#34;https://arxiv.org/pdf/2004.10340.pdf&#34;&gt;&lt;a href=&#34;https://arxiv.org/pdf/2004.10340.pdf&#34;&gt;https://arxiv.org/pdf/2004.10340.pdf&lt;/a&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start=&#34;7&#34;&gt;
&lt;li&gt;&lt;strong&gt;UCSD Birds 200&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;&lt;u&gt;Data:&lt;/u&gt; &lt;a href=&#34;http://www.vision.caltech.edu/visipedia/CUB-200.html&#34;&gt;&lt;a href=&#34;http://www.vision.caltech.edu/visipedia/CUB-200.html&#34;&gt;http://www.vision.caltech.edu/visipedia/CUB-200.html&lt;/a&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;u&gt;Paper:&lt;/u&gt; &lt;a href=&#34;https://authors.library.caltech.edu/27452/1/CUB_200_2011.pdf&#34;&gt;&lt;a href=&#34;https://authors.library.caltech.edu/27452/1/CUB_200_2011.pdf&#34;&gt;https://authors.library.caltech.edu/27452/1/CUB_200_2011.pdf&lt;/a&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start=&#34;8&#34;&gt;
&lt;li&gt;&lt;strong&gt;Birdsnap Large-scale Fine-grained Visual Categorization of Birds&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;&lt;u&gt;Data:&lt;/u&gt; &lt;a href=&#34;http://birdsnap.com/&#34;&gt;Link doesn&amp;rsquo;t work anymore.&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;u&gt;Paper:&lt;/u&gt; &lt;a href=&#34;&#34;&gt;&lt;a href=&#34;https://openaccess.thecvf.com/content_cvpr_2014/papers/Berg_Birdsnap_Large-scale_Fine-grained_2014_CVPR_paper.pdf&#34;&gt;https://openaccess.thecvf.com/content_cvpr_2014/papers/Berg_Birdsnap_Large-scale_Fine-grained_2014_CVPR_paper.pdf&lt;/a&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start=&#34;9&#34;&gt;
&lt;li&gt;&lt;strong&gt;Stanford Dogs&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;&lt;u&gt;Data:&lt;/u&gt; &lt;a href=&#34;http://vision.stanford.edu/aditya86/ImageNetDogs/&#34;&gt;&lt;a href=&#34;http://vision.stanford.edu/aditya86/ImageNetDogs/&#34;&gt;http://vision.stanford.edu/aditya86/ImageNetDogs/&lt;/a&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;u&gt;Paper:&lt;/u&gt; &lt;a href=&#34;https://people.csail.mit.edu/khosla/papers/fgvc2011.pdf&#34;&gt;&lt;a href=&#34;https://people.csail.mit.edu/khosla/papers/fgvc2011.pdf&#34;&gt;https://people.csail.mit.edu/khosla/papers/fgvc2011.pdf&lt;/a&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start=&#34;10&#34;&gt;
&lt;li&gt;&lt;strong&gt;Oxford Dogs&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;&lt;u&gt;Data:&lt;/u&gt;&lt;a href=&#34;https://www.robots.ox.ac.uk/~vgg/data/pets/&#34;&gt;&lt;a href=&#34;https://www.robots.ox.ac.uk/~vgg/data/pets/&#34;&gt;https://www.robots.ox.ac.uk/~vgg/data/pets/&lt;/a&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;u&gt;Paper:&lt;/u&gt;&lt;a href=&#34;https://www.robots.ox.ac.uk/~vgg/publications/2012/parkhi12a/parkhi12a.pdf&#34;&gt;&lt;a href=&#34;https://www.robots.ox.ac.uk/~vgg/publications/2012/parkhi12a/parkhi12a.pdf&#34;&gt;https://www.robots.ox.ac.uk/~vgg/publications/2012/parkhi12a/parkhi12a.pdf&lt;/a&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start=&#34;11&#34;&gt;
&lt;li&gt;&lt;strong&gt;Flowers&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;&lt;u&gt;Data:&lt;/u&gt;&lt;a href=&#34;https://www.robots.ox.ac.uk/~vgg/data.html&#34;&gt;Link mentioned in the paper but doesn&amp;rsquo;t work. Couldn&amp;rsquo;t find the datasest&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;u&gt;Paper:&lt;/u&gt;&lt;a href=&#34;https://www.ics.uci.edu/~welling/teaching/273ASpring09/nilsback06.pdf&#34;&gt;&lt;a href=&#34;https://www.ics.uci.edu/~welling/teaching/273ASpring09/nilsback06.pdf&#34;&gt;https://www.ics.uci.edu/~welling/teaching/273ASpring09/nilsback06.pdf&lt;/a&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Hyperopt: A tool for parameter tuning</title>
      <link>https://srishti.dev/post/1111-11-11-hyperopt/</link>
      <pubDate>Sat, 11 Nov 1111 12:38:00 -0700</pubDate>
      <guid>https://srishti.dev/post/1111-11-11-hyperopt/</guid>
      <description>&lt;p&gt;In deep learning, it is not easy to tune hyperparameters for optimal results. If we have 2 parameters (each with 3 prior desirable values), it is an easier problem. We will have possible combinations to try. However, with more parameters, the possible combinations will increase exponentially. For example, for 5 parameters, each with 4 desired values, we will have possible combinations. Manually trying each of them is not a very practical approach.&lt;/p&gt;
&lt;p&gt;hence, the question usually is &amp;ldquo;Which way should I update my hyper-parameter to reduce the loss (i.e. gradients) in order to find the optimal model architecture?&lt;/p&gt;
&lt;p&gt;Idealy, we should come up with an approach :&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;where we can search for hyperparameters (hyper-parameter search space) using a distributed compute on the cloud.&lt;/li&gt;
&lt;li&gt;intelligently optimize which of the possible combinations from the search space will give us the best results.&lt;/li&gt;
&lt;li&gt;Supports exisitng optimization techniques like Grid Search, Random Search, Bayesian Optmization etc.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Some of the exisiting tools for hyperparamter tuning are:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;RayTune (&lt;a href=&#34;https://ray.readthedocs.io/en/latest/tune.html&#34;&gt;https://ray.readthedocs.io/en/latest/tune.html&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Talos (&lt;a href=&#34;https://autonomio.github.io/talos&#34;&gt;https://autonomio.github.io/talos&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;NNI (&lt;a href=&#34;https://nni.readthedocs.io/en/latest/index.html&#34;&gt;https://nni.readthedocs.io/en/latest/index.html&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Orion (&lt;a href=&#34;https://orion.readthedocs.io/en/latest/user/pytorch.html#adapting-the-code-for-orion&#34;&gt;https://orion.readthedocs.io/en/latest/user/pytorch.html#adapting-the-code-for-orion&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Sherpa (&lt;a href=&#34;https://parameter-sherpa.readthedocs.io/en/latest/parallel/parallel-guide.html&#34;&gt;https://parameter-sherpa.readthedocs.io/en/latest/parallel/parallel-guide.html&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;TPOT (&lt;a href=&#34;https://github.com/EpistasisLab/tpot&#34;&gt;https://github.com/EpistasisLab/tpot&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Hyperopt (&lt;a href=&#34;https://github.com/hyperopt/hyperopt&#34;&gt;https://github.com/hyperopt/hyperopt&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;AWS Sagemaker Hyperparameter tuning (&lt;a href=&#34;https://aws.amazon.com/sagemaker/?hp=tile&amp;amp;so-exp=below&#34;&gt;https://aws.amazon.com/sagemaker/?hp=tile&amp;amp;so-exp=below&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;Botorch (&lt;a href=&#34;https://botorch.org/&#34;&gt;https://botorch.org/&lt;/a&gt;)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Here, we will discuss hyperopt!&lt;/p&gt;
&lt;p&gt;Hyperopt is an open-source hyperparameter tuning library written for Python. Hyperopt provides a general API for searching over hyperparameters and model types. Hyperopt offers two tuning algorithms: Random Search and the Bayesian method Tree of Parzen Estimators (TPE). To run hyperopt you define:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;the objective function&lt;/li&gt;
&lt;li&gt;the parameter space&lt;/li&gt;
&lt;li&gt;the number of experiments&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;There are both continuous and categorical methods to describe the parameters.&lt;/p&gt;
&lt;!-- &lt;img src=&#34;https://srishti.dev/img/hyperopt.png &#34; width=&#34;600&#34; height=&#34;800&#34; /&gt; --&gt;
&lt;p&gt;&lt;strong&gt;Limitation&lt;/strong&gt;:
Hyper-parameter takes a lot of time to tune the parameters if number of trials and number of epocs (iteration of the neural network) are higher (which is desired). Hence, it would be good to explore how to parallelize the tuning work.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>KL-Divergence</title>
      <link>https://srishti.dev/post/1111-11-11-kl-divergence/</link>
      <pubDate>Sat, 11 Nov 1111 12:38:00 -0700</pubDate>
      <guid>https://srishti.dev/post/1111-11-11-kl-divergence/</guid>
      <description>&lt;p&gt;&lt;strong&gt;What is KL-Divergence&lt;/strong&gt;
KL Divergence is a measure of how one probabilty distributon is different from another. Some people also call it the distance between two distributions, however, strictly speaking it is not the distance. Distance is commutative while KL-Divergence is not.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Mathematically&lt;/strong&gt;:&lt;/p&gt;
&lt;p&gt;$$ D_{KL}(P||Q) = \sum_x P(X= x) log\frac{P(X=x)}{Q(X=x)} $$&lt;/p&gt;
&lt;p&gt;$$ D_{KL}(P||Q) = \sum_x P(X= x) log\frac{P(X=x)}{Q(X=x)} $$&lt;/p&gt;
&lt;p&gt;where $\sum_x P(X= x)$ is the summation of all the values that random variable $X$ will take and $P(X= x)$ is the probabilty of that random variable. In short we can also write:&lt;/p&gt;
&lt;p&gt;$$ D_{KL}(P||Q) = \sum_x P(x) log\frac{P(x)}{Q(x)} $$&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;strong&gt;How can we generalize it to two different distributions?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Suppose we have two multivariate normal distributions defined as&lt;/p&gt;
&lt;p&gt;$$ p(x) = N(x;p_1, \Sigma_1 ) $$
$$ q(x) = N(x;p_2, \Sigma_2 ) $$&lt;/p&gt;
&lt;p&gt;where $N$ is the normal distribution, $p_1$ &amp;amp; $p_2$ are are the means and $\Sigma_1$ and $\Sigma_2$ are the covariance matrix.&lt;/p&gt;
&lt;p&gt;The multivariate normal distribution is defined as:&lt;/p&gt;
&lt;p&gt;$$ N(x;p;\Sigma_1) = \frac{1}{\sqrt{(2\pi)^{k} |\Sigma_1|}} * exp(- \frac{1}{2} (x-p)^{T} \Sigma^{-1}(x-p)) $$&lt;/p&gt;
&lt;p&gt;if the two distributions have the same distributions. Here $x$ is the vector of length $k$ x $1$ i.e. the elements are $x= [x_1, x_2&amp;hellip;x_n]^{T}$&lt;/p&gt;
&lt;p&gt;Now, if we have two distributions as mutivariate normal density, then KL Divergence between the two normal distributions is defined as:&lt;/p&gt;
&lt;p&gt;$$ D_{KL}(p(x) || q(x)) = \frac{1}{2} {  \bigg[log \frac{|\Sigma_{2}|}{|\Sigma_{1}|} - d + tr(\Sigma_2^{-1}\Sigma_1) + (p_2 - p_1)^{T} \Sigma_2^{-1}(p_2 - p_1)} \bigg] $$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Proof&lt;/strong&gt;:&lt;/p&gt;
&lt;p&gt;We know that KL Divergence between two PDFs can be expressed as:
$$ D_{KL}(p(x) || q(x)) = \sum_x P(x) log\frac{P(x)}{Q(x)} \tag{1}  $$&lt;/p&gt;
&lt;p&gt;and multi-variate normal distribution is defined like:
$$ N(x;p;\Sigma_1) = \frac{1}{\sqrt{(2\pi)^{k} |\Sigma_1|}} * exp\Big[- \frac{1}{2} (x-p_1)^{T} \Sigma^{-1}(x-p_1)\Big] \tag{2}  $$&lt;/p&gt;
&lt;!-- See \ref{1} for a how-to.  --&gt;
&lt;p&gt;&lt;em&gt;Note: $|\Sigma|$ is the determinant and is not the absolute value&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Taking log of Eq.$2$, we can write as:&lt;/p&gt;
&lt;p&gt;$$ log P(x) = - \frac{k}{2}log(2\pi) - \frac{1}{2}log(|\Sigma_1|) - \frac{1}{2} \Big[(x-p_1)^{T} \Sigma^{-1}(x-p_1)\Big]  \tag{3} $$&lt;/p&gt;
&lt;p&gt;Similarly for second probabilty distribution $Q$,
$$ log Q(x) = - \frac{k}{2}log(2\pi) - \frac{1}{2}log(|\Sigma_2|) - \frac{1}{2} \Big[(x-p_2)^{T} \Sigma^{-1}(x-p_2)\Big]  \tag{4} $$&lt;/p&gt;
&lt;p&gt;Now, Eq.$\tag{1}$ can be re-written as:
$$ D_{KL}(p(x) || q(x)) = \sum_x P(x) * [logP(x) - logQ(x)] \tag{5}  $$&lt;/p&gt;
&lt;p&gt;Substituting Eq. $3$ and Eq. $4$ in Eq. $5$, we get:&lt;/p&gt;
&lt;div&gt;
$$ 
\begin{multline} 
 D_{KL}(p(x) || q(x)) = 
\sum_x P(x) *  \\
\hspace{25mm}- \Big[\frac{k}{2}log(2\pi) - \frac{1}{2}log(|\Sigma_1|) - \frac{1}{2} \Big[(x-p_1)^{T} \Sigma^{-1}(x-p_1)\Big] \Big] \\
\hspace{-20mm} \Big[- \Big[- \frac{k}{2}log(2\pi) - \frac{1}{2}log(|\Sigma_2|) - \frac{1}{2} \Big[(x-p_2)^{T} \Sigma^{-1}(x-p_2)\Big]\Big]\Big]
\end{multline} 
$$
&lt;/div&gt;
&lt;p&gt;$$ $$&lt;/p&gt;
&lt;div&gt;
$$ 
\begin{multline} 
 D_{KL}(p(x) || q(x)) = 
\sum_x P(x) *  \\
\hspace{25mm} \xcancel{-\frac{k}{2}log(2\pi)}  \boxed{- \frac{1}{2}log(|\Sigma_1|)} - \frac{1}{2} \Big[(x-p_1)^{T} \Sigma^{-1}(x-p_1) \\
\xcancel{+\frac{k}{2}log(2\pi)} \boxed{+ \frac{1}{2}log(|\Sigma_2|)} + \frac{1}{2} \Big[(x-p_2)^{T} \Sigma^{-1}(x-p_2)\Big]
\end{multline} 
$$
&lt;/div&gt;
&lt;p&gt;We can cancel out the parts that give 0, club the boxed item in one and remaining terms in one. Hence,&lt;/p&gt;
&lt;div&gt;
$$ 
\begin{multline} 
 D_{KL}(p(x) || q(x)) = 
\sum_x P(x) *  \\
\hspace{25mm}  \Bigg[ \hspace{5mm} \boxed{+ \frac{1}{2}log(\frac{|\Sigma_2|}{|\Sigma_1|})} - \frac{1}{2} \Big[(x-p_1)^{T} \Sigma^{-1}(x-p_1) \Big] + \frac{1}{2} \Big[(x-p_2)^{T} \Sigma^{-1}(x-p_2)\Big]\Bigg]\tag{6}
\end{multline} 
$$
&lt;/div&gt;
&lt;p&gt;$P(x)$ can be split for each of the terms and hence can be written as:&lt;/p&gt;
&lt;div&gt;
$$ 
\begin{multline} 
 D_{KL}(p(x) || q(x)) = 
\sum_x P(x) * \Bigg[ \hspace{5mm} \frac{1}{2}log(\frac{|\Sigma_2|}{|\Sigma_1|})\Bigg] \\
\sum_x P(x) * \Bigg[- \frac{1}{2} \Big[(x-p_1)^{T} \Sigma^{-1}(x-p_1) \Big] \Bigg] + 
\sum_x P(x) * \Bigg[ \frac{1}{2} \Big[(x-p_2)^{T} \Sigma^{-1}(x-p_2)\Big]\Bigg] \tag{7}
\end{multline} 
$$
&lt;/div&gt;
&lt;p&gt;Now let us solve the remaining terms one by one. We will start with&lt;/p&gt;
&lt;p&gt;$$
\sum_x P(x) * \Bigg[ \frac{1}{2} \Big[(x-p_2)^{T} \Sigma_2^{-1}(x-p_2)\Big]\Bigg] \equiv E_p\Bigg[ \frac{1}{2} \Big[(x-p_2)^{T} \Sigma_2^{-1}(x-p_2)\Big]\Bigg]
$$&lt;/p&gt;
&lt;p&gt;Using properties of trace and expectation (Appendix -1), we can re-write:&lt;/p&gt;
&lt;p&gt;$$
E_p\Bigg[ \frac{1}{2} \Big[(x-p_2)^{T} \Sigma_2^{-1}(x-p_2)\Big]\Bigg]
$$ as&lt;/p&gt;
&lt;p&gt;$$
= \frac{1}{2}E_p\Bigg[ tr\Big[(x-p_2)^{T} \Sigma_2^{-1}(x-p_2)\Big]\Bigg]
= E_p\Bigg[ tr\Big[\frac{1}{2} (x-p_2)^{T} \Sigma_2^{-1}(x-p_2)\Big]\Bigg]
= E_p\Bigg[ tr\Big[\frac{1}{2}(x-p_2)(x-p_2)^{T} \Sigma_2^{-1}\Big]\Bigg]
$$&lt;/p&gt;
&lt;p&gt;Using propery __, we have&lt;/p&gt;
&lt;p&gt;$$
= tr \Bigg[\hspace{3mm}\boxed{E_p \Bigg( \Bigg[(x-p_2)(x-p_2)^{T} \Bigg]} \frac{1}{2} \Sigma_2^{-1} \Bigg) \Bigg]
$$&lt;/p&gt;
&lt;p&gt;The boxed element is nothing but a covaiance matrix $\Sigma_2$, therefore
$$
D_{KL}\Big(p(x) || q(x)\Big) =  tr \Bigg[ \Sigma_2 \frac{1}{2} \Sigma_2^{-1}] \Bigg] = tr \Bigg[ I_k \Bigg] \equiv k
$$&lt;/p&gt;
&lt;p&gt;(Equations in Latex are so tough sometimes) To be continued &amp;hellip;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Low/Few Shot Learning</title>
      <link>https://srishti.dev/post/1111-11-11-low-shot-learning-basic-concepts/</link>
      <pubDate>Sat, 11 Nov 1111 12:38:00 -0700</pubDate>
      <guid>https://srishti.dev/post/1111-11-11-low-shot-learning-basic-concepts/</guid>
      <description>&lt;p&gt;&lt;b&gt;Question&lt;/b&gt;: If a class has only two samples, can a computer make correct prediction? &lt;br&gt;
&lt;i&gt;Note: Number of samples is too less for training.&lt;/i&gt;&lt;/p&gt;
&lt;p&gt;&lt;b&gt;Approach:&lt;/b&gt; Few Shot Learning&lt;/p&gt;
&lt;p&gt;Few shot learning is a problem where we try to learn when the training data is very small. It is different from supervised learning where we train on some data and try to predict an object which belongs to a class present in training data. In few shot learning, the training data has never the sample we are predicting on.&lt;/p&gt;
&lt;p&gt;Le&amp;rsquo;s take an example. Look at Fig.1&lt;/p&gt;
&lt;figure&gt;
    &lt;img src=&#34;https://srishti.dev/img/half-training-data.jpg&#34; width=&#34;300&#34; height=&#34;400&#34; /&gt;
    &lt;figcaption&gt;
    &lt;a href=&#34;https://cs231n.github.io/classification/&#34;&gt; Fig 1. Photo via CS231 course&lt;/a&gt;
    &lt;/figcaption&gt; 
 &lt;/figure&gt;
&lt;p&gt;We train a model of a big training set of cats, dogs, mug and hat. The goal in few shot learning is not to recognize unseen cats/dogs/mugs/hats. Instead the goal is to recognize the similarity and difference between objects. After training, if we show two pairs of images (Fig.2 and Fig.3) to the model and ask &amp;ldquo;&lt;em&gt;Are the two images similar?&lt;/em&gt;&amp;quot;.&lt;/p&gt;
&lt;!DOCTYPE html&gt;
&lt;html&gt;
&lt;head&gt;
&lt;style&gt;
* {
  box-sizing: border-box;
}
.column img {
    /* width: 100px;
    height: 150px; */
    /* float: left ; */
    padding: 10px; 
}
/* Clearfix (clear floats) */
.row::after {
  content: &#34;&#34;;
  clear: both;
  display: table;
}
&lt;/style&gt;
&lt;/head&gt;
&lt;body&gt;
&lt;div class=&#34;row&#34;&gt;
    &lt;div class=&#34;column&#34;&gt;
        &lt;img src=&#34;https://srishti.dev/img/cat.png&#34; width=&#34;300&#34; height=&#34;600&#34; style=&#34;width:120%&#34;&gt;
        &lt;figcaption&gt;
            &lt;a href=&#34;https://medium.com/fenwicks/tutorial-4-demystifying-keras-dogs-vs-cats-tutorial-f7c5ea7adcf8&#34;&gt; Fig 2. Photo via Medium blog&lt;/a&gt;
        &lt;/figcaption&gt; 
    &lt;/div&gt;
    &lt;div class=&#34;column&#34;&gt;
        &lt;img src=&#34;https://srishti.dev/img/cat-dog.jpeg&#34; width=&#34;300&#34; height=&#34;600&#34; style=&#34;width:100%&#34; &gt;
        &lt;figcaption&gt;
            &lt;a href=&#34;https://cdn-images-1.medium.com/max/1600/1*EvMbMNRHm_aOf1n4tDO1Xg.jpeg&#34;&gt; Fig 3. Photo via Medium blog&lt;/a&gt;
        &lt;/figcaption&gt;  
    &lt;/div&gt;
&lt;/div&gt;
&lt;/body&gt;
&lt;/html&gt;
&lt;p&gt;Since the model has learned the similarity and difference between objects, it can tell that images in Fig.2 are same kind of objects and Fig.3 are quite different. Now, if you ask the model to &lt;em&gt;recognize&lt;/em&gt; the objects, it does not know it is cat or dog because it isn&amp;rsquo;t a part of the training data.  Hence, model can tell that these two images are similar but can&amp;rsquo;t tell what they are.&lt;/p&gt;
&lt;p&gt;Now, let&amp;rsquo;s ask another question. Let&amp;rsquo;s ask the model what is Fig. 4. We call this image: &lt;strong&gt;Query&lt;/strong&gt;&lt;/p&gt;
&lt;figure&gt;
    &lt;img src=&#34;https://srishti.dev/img/squirrel.jpg&#34; width=&#34;300&#34; height=&#34;400&#34; /&gt;
    &lt;figcaption&gt;
    &lt;a href=&#34;http://weknowyourdreams.com/single/squirrel/squirrel-09&#34;&gt; Fig 4. &#34;Query&#34; (Photo via web)&lt;/a&gt;
    &lt;/figcaption&gt; 
 &lt;/figure&gt;
&lt;p&gt;Model is unable to answer this question because it has never seen this data during training. Now, I show another 4 images to the model as shown in Fig. 5&lt;/p&gt;
&lt;figure&gt;
    &lt;img src=&#34;https://srishti.dev/img/support-set.png&#34; width=&#34;600&#34; height=&#34;800&#34; /&gt;
    &lt;figcaption&gt;
    &lt;a&gt; Fig 5. &#34;Support-Set&#34;&lt;/a&gt;
    &lt;/figcaption&gt; 
 &lt;/figure&gt;
&lt;p&gt;Now, model compares the query image with each image in the support set and believes the query is &amp;ldquo;Squirre&amp;rdquo;. These set of labelled images are called &lt;strong&gt;&amp;ldquo;Support Set&amp;rdquo;&lt;/strong&gt;.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;Jargons related to few-shot learning&lt;/p&gt;
&lt;!-- Let&#39;s define few terms before we get into more technical details: --&gt;
&lt;ul&gt;
&lt;li&gt;&lt;b&gt;k-way:&lt;/b&gt; we are given $j$ classes (e.g. 3-way means 3 classes. See Fig.1)&lt;/li&gt;
&lt;li&gt;&lt;b&gt;n-shot:&lt;/b&gt; the number of samples per class (e.g. 2-shot mean 2 sample per class. See Fig.3)&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Support set($S$):&lt;/b&gt; the samples used for learning. If we have $j$ classes and $k$ samples, then number of elements in support set is $ j * k $.&lt;/li&gt;
&lt;li&gt;&lt;b&gt;Query set ($Q$):&lt;/b&gt; the sample(s) which we are trying to identify.&lt;/li&gt;
&lt;/ul&gt;
&lt;figure&gt;
    &lt;img src=&#34;https://srishti.dev/img/support_query.png &#34; width=&#34;600&#34; height=&#34;800&#34; /&gt;
    &lt;figcaption&gt;
    &lt;a href=&#34;https://www.borealisai.com/en/blog/tutorial-2-few-shot-learning-and-meta-learning-i/&#34;&gt; Fig 3. Photo via Borealis AI blog&lt;/a&gt;
    &lt;/figcaption&gt; 
 &lt;/figure&gt;
&lt;p&gt;More formally,if we define task with $T$, then task $T_i$ is sampled from probabilty distribution over tasks&lt;/p&gt;
&lt;div&gt; $$ T_i\sim P(T) $$&lt;/div&gt;  and each task $i$ is a set given by Support $S$ and Query $Q$
&lt;div&gt;$$ T_i = \{ S_i, Q_i \} $$&lt;/div&gt;
&lt;!-- Points:
- If we already already pre-trained network on say,imagnet, it appears that if we simply use &#39;traditional&#39; transfer learning, we shold be able to achieve  better results in j-way k-shot learning. However, it doesn&#39;t necessarily happen. &lt;br /&gt; &lt;br /&gt;
We are dealing with domain transfer. First, there is the problem of domain shift. You may be in a new setting where your model cannot generalize to the new classes (classes different from the one in ImageNet for instance). Few-Shot models better deal with this setting. Second issue, in transfer learning you have to retrain part of the model (e.g. the last layer) and to do so you have to find the right number of epochs, optimizer, and hyperparams. This is very complicated to do when you have just a handful of data like in the Few-Shot setting. --&gt;
</description>
    </item>
    
    <item>
      <title>Presence-Only Geographical Priors for Fine-Grained Image Classification</title>
      <link>https://srishti.dev/post/1111-11-11-presence-only-geographical-priors-for-fine-grained-image-classification/</link>
      <pubDate>Sat, 11 Nov 1111 12:38:00 -0700</pubDate>
      <guid>https://srishti.dev/post/1111-11-11-presence-only-geographical-priors-for-fine-grained-image-classification/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Paper:&lt;/strong&gt; 
&lt;a href=&#34;https://arxiv.org/abs/1906.05272&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Objective:&lt;/strong&gt; &lt;/b&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Explores how can we use additional meta-data available to make better classification (in this case animal species).&lt;/li&gt;
&lt;li&gt;Explores how to make best use of additional meta data which comes with most images today.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Summary:&lt;/strong&gt; &lt;/b&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Knowing where a given image was taken can provide a strong prior for what objects it may contain.&lt;/li&gt;
&lt;li&gt;Paper provide a novel training loss to capture these relationship&lt;/li&gt;
&lt;li&gt;The data they assemble can have unrelated image and location dataset as long as both contain the same categories.&lt;/li&gt;
&lt;li&gt;At test time, given an image and where and when it was taken, they aim to estimate which category it contains.&lt;/li&gt;
&lt;li&gt;Location information is incorporated as bayesian spatio-temporal prior. Also, during the modelling, spatio temporal (longitude, latitude, time) are independent from the image classifier&lt;/li&gt;
&lt;li&gt;It is difficult and time consuming to have information on where and when a given category has been a.) observed to be present and b.) observed to be absent. Hence, the paper explores presence-only setting (&lt;u&gt;novelty&lt;/u&gt;)&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>The iNaturalist Species Classification and Detection Dataset</title>
      <link>https://srishti.dev/post/1111-11-11-the-inaturalist-species-classification-and-detection-dataset/</link>
      <pubDate>Sat, 11 Nov 1111 12:38:00 -0700</pubDate>
      <guid>https://srishti.dev/post/1111-11-11-the-inaturalist-species-classification-and-detection-dataset/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Paper:&lt;/strong&gt; 
&lt;a href=&#34;https://openaccess.thecvf.com/content_cvpr_2018/papers/Van_Horn_The_INaturalist_Species_CVPR_2018_paper.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Objective:&lt;/strong&gt; &lt;/b&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Focuses on species of plants and animals captured in wide variety of situations, different camera types, varying image quality, feature large class imbalance and verified by citizen scientists.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Summary:&lt;/strong&gt; &lt;/b&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;u&gt;Details:&lt;/u&gt; There are a total of 5,089 categories in the dataset, with 579,184 training images and 95,986 validation images. For the training set, the distribution of images per category follows the observation frequency of that category by the iNaturalist community. Therefore, there is a non-uniform distribution of images per category.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;u&gt;Experiments: &lt;/u&gt; Classification experiments were done using ResNet, Inception V3, Inception ResNet V2 and MobileNet&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;u&gt;Known issues: &lt;/u&gt; a.)  Doesn&amp;rsquo;t contains additional annotations such as sex and life stage attributes, habitat tags, and pixel level labels for the four super-classes that were challenging to annotate. b.) Need of an efficient algorithm that works when the test set contains classes that were never seen during training.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>The Unreasonable Effectiveness of Noisy Data for Fine-Grained Recognition</title>
      <link>https://srishti.dev/post/1111-11-11-the-unreasonable-effectiveness-of-noisy-data-for-fine-grained-recognition/</link>
      <pubDate>Sat, 11 Nov 1111 12:38:00 -0700</pubDate>
      <guid>https://srishti.dev/post/1111-11-11-the-unreasonable-effectiveness-of-noisy-data-for-fine-grained-recognition/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Paper:&lt;/strong&gt; 
&lt;a href=&#34;https://research.google/pubs/pub45605/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Objective:&lt;/strong&gt; &lt;/b&gt;
Leverage free, noisy data from the web to train effective models of fine-grained recognition.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Summary:&lt;/strong&gt; &lt;/b&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Interesting paper on using noisy data from the web.&lt;/li&gt;
&lt;li&gt;They sample images directly from Google search, using all returned images as images for a given category. For L-Bird and L-Butterfly, queries are for the scientific name of the category, and for L-Aircraft and L-Dog queries are simply for the category name (e.g.“Boeing 737-200” or “Pembroke Welsh Corgi”).&lt;/li&gt;
&lt;li&gt;Active learning-based approach to collect the data.&lt;/li&gt;
&lt;li&gt;The active learning begins by training a classifier on a seed set of input images and labels (i.e.the Stanford Dogs training set), then proceeds by  iteratively  picking  a  set  of  images  to  annotate,  obtaining  labels  with  human  annotators,  and  re-training  the  classifier.&lt;/li&gt;
&lt;li&gt;Inception V3 is the base classifier&lt;/li&gt;
&lt;li&gt;To avoid images overlap between GT and web images, aggressive duplication procedure with all ground truth test sets and their corresponding web images is performed using a SOTA for learning similarity metric between images.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Questions:&lt;/strong&gt; &lt;/b&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;How reliable are the search results for a single category from web, otherwise there are a lot of False Positives being introduced.&lt;/li&gt;
&lt;li&gt;Is it limited to extracting category information or do we need extra labels like position, time etc.&lt;/li&gt;
&lt;li&gt;How much human annotators are required to be involved or are they even involved?&lt;/li&gt;
&lt;li&gt;Since algo queries user to label data from time to time; does this query apply on a subset of the images to verify the annotations or create annotations.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>We Have So Much In Common: Modeling Semantic Relational Set Abstractions In Videos</title>
      <link>https://srishti.dev/post/1111-11-11-we-have-so-much-in-common/</link>
      <pubDate>Sat, 11 Nov 1111 12:38:00 -0700</pubDate>
      <guid>https://srishti.dev/post/1111-11-11-we-have-so-much-in-common/</guid>
      <description>&lt;p&gt;&lt;strong&gt;General Terms:&lt;/strong&gt; &lt;/b&gt;
Semantic: Semantics is the study of relationship between words and how we can draw meaning from words. Example, A child could be called a child, kid, boy, girl, son, daughter. Sematic relations connect up entities in a text. Semantic relation are at the cross-road between knowledge and language and tTogether with entities make up good chunk of the meaning of the text.&lt;/p&gt;
&lt;p&gt;To explore, how sets of group of words are linked by means of sevaeral semantic relations, one can look at 
&lt;a href=&#34;http://wordnetweb.princeton.edu/perl/webwn&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;WordNet&lt;/a&gt;.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;strong&gt;Gist:&lt;/strong&gt; &lt;/b&gt;
For humans it is easy to identify how two events are related by looking at them. Inherently, we can decompose two events in general abstract meaning and see which of the abstract meaning are similar. Example, when we see a.) a human being typing on a computer b.) a GIF with a mouse typing on a calculator, we can relate the hand movements and object being typed upon. We can relate that both activities are related to pushing some buttons and seeing a result on screen and hence can see the similarities in the two activities. It is a result of underlying human decision making abilty. Computers can&amp;rsquo;t do the same.&lt;/p&gt;
&lt;p&gt;The paper proposes &amp;ldquo;an approach for learning semantic relational set abstractions on videos&amp;rdquo;. &amp;ldquo;semantic relational set abstractions&amp;rdquo; should be read in two parts &amp;ldquo;semantic&amp;rdquo; and &amp;ldquo;relational set abstractions&amp;rdquo; for better understanding. It simply says that if we can find the relational sets between different videos (e.g. similar images at different view angles) and get an abstract meaning from these relational sets, the proposed method can learn the semantic (meaning) between these abstractions. If we can do so, is will be easier to tell which images are similar, which are different and how they are similar/different.&lt;/p&gt;
&lt;p&gt;More formally, as the paper puts it, this allows our model to perform cognitive tasks such as:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;set abstraction (which general concept is in common among a set of videos?)&lt;/li&gt;
&lt;li&gt;set completion (which new video goes well with the set?), and&lt;/li&gt;
&lt;li&gt;odd one out detection (which video does not belong to the set?)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Datsets used:&lt;/strong&gt; &lt;/b&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;K400 (Kinect 400)&lt;/li&gt;
&lt;li&gt;Multi-Monets in Time&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Accessing Windows File From Linux Subshell in Windows</title>
      <link>https://srishti.dev/post/1111-11-11-accessing-windows-from-linux/</link>
      <pubDate>Sat, 11 Nov 1111 01:25:06 -0700</pubDate>
      <guid>https://srishti.dev/post/1111-11-11-accessing-windows-from-linux/</guid>
      <description>&lt;p&gt;If you are using Ubuntu subshell in Windows (I use the one available in Microsoft Store), this is for you. There are times, where you may need to access the files saved on your windows in the Linux subshell.
All you need to do is mount your drive. You may need to access the subshell via administrative privileges. For example, if you have the file in C: Drive, use the following:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cd /mnt/c
ls
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The same can be used to access data from external hard disk too. If the external disk is in D: Drive, the command would simply change to:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cd /mnt/d
ls
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
  </channel>
</rss>
