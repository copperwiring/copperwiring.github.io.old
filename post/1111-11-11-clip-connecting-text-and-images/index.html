<!DOCTYPE html><html lang="en-us" >

<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="generator" content="Source Themes Academic 4.8.0">

  

  
  
  
  
  
    
    
    
  
  

  <meta name="author" content="Srishti Yadav">

  
  
  
    
  
  <meta name="description" content="Paper: Learning Transferable Visual Models From Natural Language Supervision
  PDF: Learning Transferable Visual Models From Natural Language Supervision
  Blog: CLIP: Connecting Text and Images
  General Terms:">

  
  <link rel="alternate" hreflang="en-us" href="https://srishti.dev/post/1111-11-11-clip-connecting-text-and-images/">

  


  
  
  
  <meta name="theme-color" content="#3f51b5">
  

  
  
  
  <script src="/js/mathjax-config.js"></script>
  

  
  
  
  
    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css" integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.0-1/css/all.min.css" integrity="sha256-4w9DunooKSr3MFXHXWyFER38WmPdm361bQS/2KUWZbU=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.css" integrity="sha256-Vzbj7sDDS/woiFS3uNKo8eIuni59rjyNGtXfstRzStA=" crossorigin="anonymous">

    
    
    
      
    
    
      
      
        
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/github.min.css" crossorigin="anonymous" title="hl-light">
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/dracula.min.css" crossorigin="anonymous" title="hl-dark" disabled>
        
      
    

    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.5.1/leaflet.css" integrity="sha256-SHMGCYmST46SoyGgo4YR/9AlK1vf3ff84Aq9yK4hdqM=" crossorigin="anonymous">
    

    

    
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/lazysizes/5.1.2/lazysizes.min.js" integrity="sha256-Md1qLToewPeKjfAHU1zyPwOutccPAm5tahnaw7Osw0A=" crossorigin="anonymous" async></script>
      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js" integrity="" crossorigin="anonymous" async></script>
      
    
      

      
      

      
    

  

  
  
  
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Montserrat:400,700%7CRoboto:400,400italic,700%7CRoboto+Mono&display=swap">
  

  
  
  
  
  <link rel="stylesheet" href="/css/academic.css">

  




  


  
  

  

  <link rel="manifest" href="/index.webmanifest">
  <link rel="icon" type="image/png" href="/images/icon_hu83f24c6794b0c645cb0fb9e8b29ab31d_111941_32x32_fill_lanczos_center_2.png">
  <link rel="apple-touch-icon" type="image/png" href="/images/icon_hu83f24c6794b0c645cb0fb9e8b29ab31d_111941_192x192_fill_lanczos_center_2.png">

  <link rel="canonical" href="https://srishti.dev/post/1111-11-11-clip-connecting-text-and-images/">

  
  
  
  
  
  
  
    
    
  
  
  <meta property="twitter:card" content="summary">
  
  <meta property="og:site_name" content="Srishti Yadav">
  <meta property="og:url" content="https://srishti.dev/post/1111-11-11-clip-connecting-text-and-images/">
  <meta property="og:title" content="CLIP: Connecting Text and Images | Srishti Yadav">
  <meta property="og:description" content="Paper: Learning Transferable Visual Models From Natural Language Supervision
  PDF: Learning Transferable Visual Models From Natural Language Supervision
  Blog: CLIP: Connecting Text and Images
  General Terms:"><meta property="og:image" content="https://srishti.dev/images/icon_hu83f24c6794b0c645cb0fb9e8b29ab31d_111941_512x512_fill_lanczos_center_2.png">
  <meta property="twitter:image" content="https://srishti.dev/images/icon_hu83f24c6794b0c645cb0fb9e8b29ab31d_111941_512x512_fill_lanczos_center_2.png"><meta property="og:locale" content="en-us">
  
    
      <meta property="article:published_time" content="1111-11-11T12:38:00-07:00">
    
    <meta property="article:modified_time" content="1111-11-11T12:38:00-07:00">
  

  


    






  




<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://srishti.dev/post/1111-11-11-clip-connecting-text-and-images/"
  },
  "headline": "CLIP: Connecting Text and Images",
  
  "datePublished": "1111-11-11T12:38:00-07:00",
  "dateModified": "1111-11-11T12:38:00-07:00",
  
  "author": {
    "@type": "Person",
    "name": "Srishti Yadav"
  },
  
  "publisher": {
    "@type": "Organization",
    "name": "Srishti Yadav",
    "logo": {
      "@type": "ImageObject",
      "url": "https://srishti.dev/images/icon_hu83f24c6794b0c645cb0fb9e8b29ab31d_111941_192x192_fill_lanczos_center_2.png"
    }
  },
  "description": "Paper: Learning Transferable Visual Models From Natural Language Supervision\n  PDF: Learning Transferable Visual Models From Natural Language Supervision\n  Blog: CLIP: Connecting Text and Images\n  General Terms:"
}
</script>

  

  


  


  





  <title>CLIP: Connecting Text and Images | Srishti Yadav</title>

</head>

<body id="top" data-spy="scroll" data-offset="70" data-target="#TableOfContents" >

  <aside class="search-results" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Search</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        
        
        
      </div>

    </section>
    <section class="section-search-results">

      <div id="search-hits">
        
      </div>

    </section>
  </div>
</aside>


  







<nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id="navbar-main">
  <div class="container">

    
    <div class="d-none d-lg-inline-flex">
      <a class="navbar-brand" href="/">Srishti Yadav</a>
    </div>
    

    
    <button type="button" class="navbar-toggler" data-toggle="collapse"
            data-target="#navbar-content" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
    <span><i class="fas fa-bars"></i></span>
    </button>
    

    
    <div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none">
      <a class="navbar-brand" href="/">Srishti Yadav</a>
    </div>
    

    
    
    <div class="navbar-collapse main-menu-item collapse justify-content-start" id="navbar-content">

      
      <ul class="navbar-nav d-md-inline-flex">
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#about"><span>Home</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        

        <li class="nav-item">
          <a class="nav-link  active" href="/post/"><span>Posts</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        

        <li class="nav-item">
          <a class="nav-link " href="/news/"><span>News</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        

        <li class="nav-item">
          <a class="nav-link " href="/reading_list/"><span>Books</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        

        <li class="nav-item">
          <a class="nav-link " href="/files/cv.pdf"><span>CV</span></a>
        </li>

        
        

      

        
      </ul>
    </div>

    <ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2">
      

      

      

    </ul>

  </div>
</nav>


  <article class="article">

  












  

  
  
  
<div class="article-container pt-3">
  <h1>CLIP: Connecting Text and Images</h1>

  

  
    


<div class="article-metadata">

  
  

  
  <span class="article-date">
    
    
      
    
    Nov 11, 1111
  </span>
  

  

  
  <span class="middot-divider"></span>
  <span class="article-reading-time">
    4 min read
  </span>
  

  
  
  

  
  

</div>

    














  
</div>



  <div class="article-container">

    <div class="article-style">
      <p><img src="/img/clip-openai.png" width="920" height="720" /></p>
<p><strong>Paper</strong>: Learning Transferable Visual Models From Natural Language Supervision</p>
<ul>
<li>
<p>PDF: 
<a href="https://cdn.openai.com/papers/Learning_Transferable_Visual_Models_From_Natural_Language_Supervision.pdf" target="_blank" rel="noopener">Learning Transferable Visual Models From Natural Language Supervision</a></p>
</li>
<li>
<p>Blog: 
<a href="https://openai.com/blog/clip/" target="_blank" rel="noopener">CLIP: Connecting Text and Images</a></p>
</li>
</ul>
<p><strong>General Terms:</strong> </b></p>
<ol>
<li>
<p><u>Contrastive Learning</u>: It is based on the intuition that you can contrast/differentiate between similar and dissimilar things. In machine learning model, we formulate this as a task of finding similar and dissimilar things i.e. the model should be able to classify between similar and dissimilar images</p>
</li>
<li>
<p><u>Zero-shot Learning </u>: Zero-shot learning is a problem, where at test stage,the model/learner aims to recognize objects whose instances maynot have been seen during training. To learn about various works that have been done in this space (atleast till 2018), this paper provides good details: 
<a href="https://arxiv.org/pdf/1707.00600.pdf" target="_blank" rel="noopener">Zero-shot learningâ€”a comprehensive evaluation of the good, the bad and the ugly</a>.</p>
</li>
</ol>
<hr>
<p>The paper describes an approach where it takes a large dataset of image text pair and tries to learn a model that scores whether a image and text could co-occur. This is learned over a large dataset.</p>
<p><strong>Question: How to do classification zero shot i.e. without any training?</strong></p>
<p>Given a specified classification task where there are some images and some labels and you are supposed to evaluate based on our prediction, you will embed all text labels into vectors and images into vectors and them compare the score of their cross product. The pairing that is going to give the highest score is going to be the prediction label.</p>
 <figure>
    <img src="/img/clip-architecture.png" width="920" height="720"/>
    <figcaption>
    <a href="https://cdn.openai.com/papers/Learning_Transferable_Visual_Models_From_Natural_Language_Supervision.pdf"> Fig 1. Photo via Open AI Paper</a>
    </figcaption> 
 </figure>
<p>In Fig.1, $N$ is the size of images associated with some text. $T_i$ is the encoding for the entire text string.</p>
<p><strong>Why is it zero shot?</strong>
This model is zero shot because it refers to the number of lables we see for training. In this architecture, for a particular evaluation task, you do not train. You take the train weights(and give some bias). THis notion of zero shot is in line with GPT notion of zero shot i.e. you do a lot of training -  you don&rsquo;t care what is happenng at the pre-training stage</p>
<p><strong>Dataset:</strong></p>
<ul>
<li>Called WiT Dataset (WebImageText)</li>
<li>400 million (image, text) pairs by searching over text queries</li>
<li>500,000 text queries:
<ul>
<li>All words occuring atleast 100 times in the English version of Wikipedia + WordNet synsets + some details</li>
</ul>
</li>
<li>Cap at a maximum of 20000 (image, text) pairs per query</li>
<li>Total wordcount similar to WebText dataset used by GPT-2</li>
</ul>
<p><u>Note</u>: There is a difference between queries and labels.
Example, when you give a query &lsquo;dog&rsquo; on google image, it will come with all sorts of images of &lsquo;dog&rsquo; and each such image will come associated with a (paired)text. These texts can be in the form of alt text or title of the page.</p>
 <figure>
    <img src="/img/clip-pretraining.png" width="920" height="720"/>
    <figcaption>
    <a href="https://cdn.openai.com/papers/Learning_Transferable_Visual_Models_From_Natural_Language_Supervision.pdf"> Fig 2. Photo via Open AI Paper</a>
    </figcaption> 
 </figure>
<p>Fig.2 shows various methods authors use to test which pre-training method is better. We know there are various training method that connects image and language. Most vanilla verison is captioning (language modelling with images). In this paper, authors try a variety of pre-training methods. First for captioning - they found it is not very compute efficient. Second, they tried BoW (bag of words) and they find that they scale better. However, the best they found was contrastive learning.</p>
<p><strong>Model architecture:</strong></p>
<p><u> Image Encode </u></p>
<ul>
<li>ResNet
<ul>
<li>Modifications: anti-aliased max pooling; final global pooling by QK Value (QKV) attention</li>
<li>Scaling model size by allocating compute equally (to width, depth, input resolution)
OR</li>
</ul>
</li>
<li>Vision Transformer</li>
</ul>
<br/>
<p><u> Text Encoder </u></p>
<ul>
<li>Transformer text encoder
<ul>
<li>scaling model size by width only; do not scale depth</li>
</ul>
</li>
</ul>
<br/>
<u> Other parameters </u>
<ul>
<li>$N$ x $N$ affinity matrix. Symmetric Contrastic Loss</li>
<li>Temperature $t$ is initialized to 0.07 but allowed to learn</li>
<li>Adam with decoupled weight decay ($adam_w$). Cosine LR.</li>
<li>Batch size 32768 (this is huge!)</li>
</ul>
<!DOCTYPE html>
<html>
<head>
<style>
* {
  box-sizing: border-box;
}
.column img {
    width: 100%;
    float: left ;
    margin: 0;
}
.column {
    width: 50%;
    float: left ;
    padding: 10px; 
}
.row::after {
  content: "";
  clear: both;
  display: table;
}
</style>
</head>
<body>
<div class="row">
    <div class="column">
        <img src="/img/clip-prompt-engineering-2.png"/>
    </div>
    <div class="column">
        <img src="/img/clip-prompt-engineering.png" />
    </div>
</div>
<div class="row">
    <div class="column">
        <figcaption>
            <a href="https://cdn.openai.com/papers/Learning_Transferable_Visual_Models_From_Natural_Language_Supervision.pdf"> Fig 3a. Photo via Open AI Paper</a>
        </figcaption> 
    </div>
    <div class="column">
        <figcaption>
            <a href="https://cdn.openai.com/papers/Learning_Transferable_Visual_Models_From_Natural_Language_Supervision.pdf"> Fig 3b. Photo via Open AI Paper</a>
        </figcaption> 
    </div>
</div>
</body>
</html>
 <figure>
 </figure>
<p><strong>Prompt Engineering:</strong></p>
<p>From Fig 3.a, we can see that in addition to texts, authors have also experimented using prompts wth texts i.e. you can embed the label itself but you can also embed string with a customized prompt ahead of it.  E.g. &lsquo;a <em>photo</em> of &lsquo;, &lsquo;a <em>bag</em> of&rsquo;. Fig 3.b shows that authors show that prompts work better than raw label (+15%)</p>
<p><strong>Performance:</strong></p>
 <figure>
    <img src="/img/clip-zero-shot-performance.png"/>
    <figcaption>
    <a href="https://cdn.openai.com/papers/Learning_Transferable_Visual_Models_From_Natural_Language_Supervision.pdf"> Fig 4. Photo via Open AI Paper from Appendix Section</a>
    </figcaption> 
 </figure>
<p>For ImageNet (1.2 million), vanilla ResNet50 gives accuracy of 56.3% <cite>[1]</cite> and zero-shot (CLIP model) gives 59.6%.(see last column of first row in Fig.4). Model (<em>L/14-336x</em>) that gives classification accuracy close to ResNet-50 uses a lot more compute (see last column, last row in Fig.4). L/14-336x means 14 large with input resolution 336px. Hence, to surpass a supervised ResNet, you need a lot bigger model</p>
<br/>
<p>[1] : <a href="https://arxiv.org/pdf/1811.06992.pdf">https://arxiv.org/pdf/1811.06992.pdf</a></p>
<p><u>Acknowledment:</u> Thanks to the discussion in TTIC reading group which introduced me to this paper.</p>

    </div>

    





<div class="article-tags">
  
  <a class="badge badge-light" href="/tag/technical/">technical</a>
  
</div>



<div class="share-box" aria-hidden="true">
  <ul class="share">
    
  </ul>
</div>












  
  





  
    
    
    
      
    
    
    
    <div class="media author-card content-widget-hr">
      
        
        <img class="avatar mr-3 avatar-circle" src="/author/srishti-yadav/avatar_hu9d788c3a7095d2e8067c7cba9a8e0ac1_2405876_270x270_fill_q90_lanczos_center.jpg" alt="Srishti Yadav">
      

      <div class="media-body">
        <h5 class="card-title"><a href="https://srishti.dev">Srishti Yadav</a></h5>
        <h6 class="card-subtitle">ML Researcher</h6>
        <p class="card-text">My research interest include applying computationally intensive machine learning algorithm to computer vision algoritmns.</p>
        <ul class="network-icon" aria-hidden="true">
  
    
    
    
      
    
    
    
    
    
    <li>
      <a href="mailto:srishtiy@sfu.ca" >
        <i class="fas fa-envelope"></i>
      </a>
    </li>
  
    
    
    
    
    
    
    
      
    
    <li>
      <a href="https://scholar.google.com/citations?user=LW9fQacAAAAJ&amp;hl=en" target="_blank" rel="noopener">
        <i class="ai ai-google-scholar"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://linkedin.com/in/srishti-yadav" target="_blank" rel="noopener">
        <i class="fab fa-linkedin"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://github.com/copperwiring" target="_blank" rel="noopener">
        <i class="fab fa-github"></i>
      </a>
    </li>
  
    
    
    
    
    
    
    
      
    
    <li>
      <a href="/files/cv.pdf" >
        <i class="ai ai-cv"></i>
      </a>
    </li>
  
</ul>

      </div>
    </div>
  












  
  
  <div class="article-widget content-widget-hr">
    <h3>Related</h3>
    <ul>
      
      <li><a href="/post/1111-11-11-presence-only-geographical-priors-for-fine-grained-image-classification/">Presence-Only Geographical Priors for Fine-Grained Image Classification</a></li>
      
      <li><a href="/post/1111-11-11-we-have-so-much-in-common/">We Have So Much In Common: Modeling Semantic Relational Set Abstractions In Videos</a></li>
      
      <li><a href="/post/1111-11-11-the-unreasonable-effectiveness-of-noisy-data-for-fine-grained-recognition/">The Unreasonable Effectiveness of Noisy Data for Fine-Grained Recognition</a></li>
      
      <li><a href="/post/1111-11-11-kl-divergence/">KL-Divergence</a></li>
      
      <li><a href="/post/1111-11-11-low-shot-learning-basic-concepts/">Low/Few Shot Learning</a></li>
      
    </ul>
  </div>
  



  </div>
</article>

      

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js" integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js" integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.js" integrity="sha256-yt2kYMy0w8AbtF89WXb2P1rfjcP/HTHLT7097U8Y5b8=" crossorigin="anonymous"></script>

      

      
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/highlight.min.js" integrity="sha256-eOgo0OtLL4cdq7RdwRUiGKLX9XsIJ7nGhWEKbohmVAQ=" crossorigin="anonymous"></script>
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/languages/r.min.js"></script>
        
      

    

    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.5.1/leaflet.js" integrity="sha256-EErZamuLefUnbMBQbsEqu1USa+btR2oIlCpBJbyD4/g=" crossorigin="anonymous"></script>
    

    
    
    <script>const code_highlighting = true;</script>
    

    
    
    <script>const isSiteThemeDark = false;</script>
    

    

    
    

    

    
    

    
    

    
    

    
    

    
    
    
    
    
    
    
    
    
    
    
    
    <script src="/js/academic.min.6f7ce8be710290b8c431bbc97f405d15.js"></script>

    






  
  
  <div class="container">
    <footer class="site-footer">
  

  <p class="powered-by">
    
  </p>

  
  






  <p class="powered-by">
    Powered by the
    <a href="https://sourcethemes.com/academic/" target="_blank" rel="noopener">Academic theme</a> for
    <a href="https://gohugo.io" target="_blank" rel="noopener">Hugo</a>.

    
    <span class="float-right" aria-hidden="true">
      <a href="#" class="back-to-top">
        <span class="button_icon">
          <i class="fas fa-chevron-up fa-2x"></i>
        </span>
      </a>
    </span>
    
  </p>

</footer>

  </div>
  

  
<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        <pre><code class="tex hljs"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

</body>
</html>
