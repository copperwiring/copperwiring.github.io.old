<!DOCTYPE html><html lang="en-us" >

<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="generator" content="Source Themes Academic 4.8.0">

  

  
  
  
  
  
    
    
    
  
  

  <meta name="author" content="Srishti Yadav">

  
  
  
    
  
  <meta name="description" content="What is KL-Divergence KL Divergence is a measure of how one probabilty distributon is different from another. Some people also call it the distance between two distributions, however, strictly speaking it is not the distance.">

  
  <link rel="alternate" hreflang="en-us" href="https://srishti.dev/post/1111-11-11-kl-divergence/">

  


  
  
  
  <meta name="theme-color" content="#3f51b5">
  

  
  
  
  <script src="/js/mathjax-config.js"></script>
  

  
  
  
  
    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css" integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.0-1/css/all.min.css" integrity="sha256-4w9DunooKSr3MFXHXWyFER38WmPdm361bQS/2KUWZbU=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.css" integrity="sha256-Vzbj7sDDS/woiFS3uNKo8eIuni59rjyNGtXfstRzStA=" crossorigin="anonymous">

    
    
    
      
    
    
      
      
        
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/github.min.css" crossorigin="anonymous" title="hl-light">
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/dracula.min.css" crossorigin="anonymous" title="hl-dark" disabled>
        
      
    

    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.5.1/leaflet.css" integrity="sha256-SHMGCYmST46SoyGgo4YR/9AlK1vf3ff84Aq9yK4hdqM=" crossorigin="anonymous">
    

    

    
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/lazysizes/5.1.2/lazysizes.min.js" integrity="sha256-Md1qLToewPeKjfAHU1zyPwOutccPAm5tahnaw7Osw0A=" crossorigin="anonymous" async></script>
      
    
      

      
      

      
    
      

      
      

      
    
      

      
      

      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js" integrity="" crossorigin="anonymous" async></script>
      
    
      

      
      

      
    

  

  
  
  
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Montserrat:400,700%7CRoboto:400,400italic,700%7CRoboto+Mono&display=swap">
  

  
  
  
  
  <link rel="stylesheet" href="/css/academic.css">

  




  


  
  

  

  <link rel="manifest" href="/index.webmanifest">
  <link rel="icon" type="image/png" href="/images/icon_hu83f24c6794b0c645cb0fb9e8b29ab31d_111941_32x32_fill_lanczos_center_2.png">
  <link rel="apple-touch-icon" type="image/png" href="/images/icon_hu83f24c6794b0c645cb0fb9e8b29ab31d_111941_192x192_fill_lanczos_center_2.png">

  <link rel="canonical" href="https://srishti.dev/post/1111-11-11-kl-divergence/">

  
  
  
  
  
  
  
    
    
  
  
  <meta property="twitter:card" content="summary">
  
  <meta property="og:site_name" content="Srishti Yadav">
  <meta property="og:url" content="https://srishti.dev/post/1111-11-11-kl-divergence/">
  <meta property="og:title" content="KL-Divergence | Srishti Yadav">
  <meta property="og:description" content="What is KL-Divergence KL Divergence is a measure of how one probabilty distributon is different from another. Some people also call it the distance between two distributions, however, strictly speaking it is not the distance."><meta property="og:image" content="https://srishti.dev/images/icon_hu83f24c6794b0c645cb0fb9e8b29ab31d_111941_512x512_fill_lanczos_center_2.png">
  <meta property="twitter:image" content="https://srishti.dev/images/icon_hu83f24c6794b0c645cb0fb9e8b29ab31d_111941_512x512_fill_lanczos_center_2.png"><meta property="og:locale" content="en-us">
  
    
      <meta property="article:published_time" content="1111-11-11T12:38:00-07:00">
    
    <meta property="article:modified_time" content="1111-11-11T12:38:00-07:00">
  

  


    






  




<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://srishti.dev/post/1111-11-11-kl-divergence/"
  },
  "headline": "KL-Divergence",
  
  "datePublished": "1111-11-11T12:38:00-07:00",
  "dateModified": "1111-11-11T12:38:00-07:00",
  
  "author": {
    "@type": "Person",
    "name": "Srishti Yadav"
  },
  
  "publisher": {
    "@type": "Organization",
    "name": "Srishti Yadav",
    "logo": {
      "@type": "ImageObject",
      "url": "https://srishti.dev/images/icon_hu83f24c6794b0c645cb0fb9e8b29ab31d_111941_192x192_fill_lanczos_center_2.png"
    }
  },
  "description": "What is KL-Divergence KL Divergence is a measure of how one probabilty distributon is different from another. Some people also call it the distance between two distributions, however, strictly speaking it is not the distance."
}
</script>

  

  


  


  





  <title>KL-Divergence | Srishti Yadav</title>

</head>

<body id="top" data-spy="scroll" data-offset="70" data-target="#TableOfContents" >

  <aside class="search-results" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Search</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        
        
        
      </div>

    </section>
    <section class="section-search-results">

      <div id="search-hits">
        
      </div>

    </section>
  </div>
</aside>


  







<nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id="navbar-main">
  <div class="container">

    
    <div class="d-none d-lg-inline-flex">
      <a class="navbar-brand" href="/">Srishti Yadav</a>
    </div>
    

    
    <button type="button" class="navbar-toggler" data-toggle="collapse"
            data-target="#navbar-content" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
    <span><i class="fas fa-bars"></i></span>
    </button>
    

    
    <div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none">
      <a class="navbar-brand" href="/">Srishti Yadav</a>
    </div>
    

    
    
    <div class="navbar-collapse main-menu-item collapse justify-content-start" id="navbar-content">

      
      <ul class="navbar-nav d-md-inline-flex">
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#about"><span>Home</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        

        <li class="nav-item">
          <a class="nav-link  active" href="/post/"><span>Posts</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        

        <li class="nav-item">
          <a class="nav-link " href="/publication/"><span>Publications</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        

        <li class="nav-item">
          <a class="nav-link " href="/news/"><span>News</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        

        <li class="nav-item">
          <a class="nav-link " href="/reading_list/"><span>Books</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        

        <li class="nav-item">
          <a class="nav-link " href="/files/cv.pdf"><span>CV</span></a>
        </li>

        
        

      

        
      </ul>
    </div>

    <ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2">
      

      
      <li class="nav-item dropdown theme-dropdown">
        <a href="#" class="nav-link js-theme-selector" data-toggle="dropdown" aria-haspopup="true">
          <i class="fas fa-palette" aria-hidden="true"></i>
        </a>
        <div class="dropdown-menu">
          <a href="#" class="dropdown-item js-set-theme-light">
            <span>Light</span>
          </a>
          <a href="#" class="dropdown-item js-set-theme-dark">
            <span>Dark</span>
          </a>
          <a href="#" class="dropdown-item js-set-theme-auto">
            <span>Automatic</span>
          </a>
        </div>
      </li>
      

      

    </ul>

  </div>
</nav>


  <article class="article">

  












  

  
  
  
<div class="article-container pt-3">
  <h1>KL-Divergence</h1>

  

  
    


<div class="article-metadata">

  
  

  
  <span class="article-date">
    
    
      
    
    Nov 11, 1111
  </span>
  

  

  
  <span class="middot-divider"></span>
  <span class="article-reading-time">
    3 min read
  </span>
  

  
  
  

  
  

</div>

    














  
</div>



  <div class="article-container">

    <div class="article-style">
      <p><strong>What is KL-Divergence</strong>
KL Divergence is a measure of how one probabilty distributon is different from another. Some people also call it the distance between two distributions, however, strictly speaking it is not the distance. Distance is commutative while KL-Divergence is not.</p>
<p><strong>Mathematically</strong>:</p>
<p>$$ D_{KL}(P||Q) = \sum_x P(X= x) log\frac{P(X=x)}{Q(X=x)} $$</p>
<p>$$ D_{KL}(P||Q) = \sum_x P(X= x) log\frac{P(X=x)}{Q(X=x)} $$</p>
<p>where $\sum_x P(X= x)$ is the summation of all the values that random variable $X$ will take and $P(X= x)$ is the probabilty of that random variable. In short we can also write:</p>
<p>$$ D_{KL}(P||Q) = \sum_x P(x) log\frac{P(x)}{Q(x)} $$</p>
<hr>
<p><strong>How can we generalize it to two different distributions?</strong></p>
<p>Suppose we have two multivariate normal distributions defined as</p>
<p>$$ p(x) = N(x;p_1, \Sigma_1 ) $$
$$ q(x) = N(x;p_2, \Sigma_2 ) $$</p>
<p>where $N$ is the normal distribution, $p_1$ &amp; $p_2$ are are the means and $\Sigma_1$ and $\Sigma_2$ are the covariance matrix.</p>
<p>The multivariate normal distribution is defined as:</p>
<p>$$ N(x;p;\Sigma_1) = \frac{1}{\sqrt{(2\pi)^{k} |\Sigma_1|}} * exp(- \frac{1}{2} (x-p)^{T} \Sigma^{-1}(x-p)) $$</p>
<p>if the two distributions have the same distributions. Here $x$ is the vector of length $k$ x $1$ i.e. the elements are $x= [x_1, x_2&hellip;x_n]^{T}$</p>
<p>Now, if we have two distributions as mutivariate normal density, then KL Divergence between the two normal distributions is defined as:</p>
<p>$$ D_{KL}(p(x) || q(x)) = \frac{1}{2} {  \bigg[log \frac{|\Sigma_{2}|}{|\Sigma_{1}|} - d + tr(\Sigma_2^{-1}\Sigma_1) + (p_2 - p_1)^{T} \Sigma_2^{-1}(p_2 - p_1)} \bigg] $$</p>
<p><strong>Proof</strong>:</p>
<p>We know that KL Divergence between two PDFs can be expressed as:
$$ D_{KL}(p(x) || q(x)) = \sum_x P(x) log\frac{P(x)}{Q(x)} \tag{1}  $$</p>
<p>and multi-variate normal distribution is defined like:
$$ N(x;p;\Sigma_1) = \frac{1}{\sqrt{(2\pi)^{k} |\Sigma_1|}} * exp\Big[- \frac{1}{2} (x-p_1)^{T} \Sigma^{-1}(x-p_1)\Big] \tag{2}  $$</p>
<!-- See \ref{1} for a how-to.  -->
<p><em>Note: $|\Sigma|$ is the determinant and is not the absolute value</em></p>
<p>Taking log of Eq.$2$, we can write as:</p>
<p>$$ log P(x) = - \frac{k}{2}log(2\pi) - \frac{1}{2}log(|\Sigma_1|) - \frac{1}{2} \Big[(x-p_1)^{T} \Sigma^{-1}(x-p_1)\Big]  \tag{3} $$</p>
<p>Similarly for second probabilty distribution $Q$,
$$ log Q(x) = - \frac{k}{2}log(2\pi) - \frac{1}{2}log(|\Sigma_2|) - \frac{1}{2} \Big[(x-p_2)^{T} \Sigma^{-1}(x-p_2)\Big]  \tag{4} $$</p>
<p>Now, Eq.$\tag{1}$ can be re-written as:
$$ D_{KL}(p(x) || q(x)) = \sum_x P(x) * [logP(x) - logQ(x)] \tag{5}  $$</p>
<p>Substituting Eq. $3$ and Eq. $4$ in Eq. $5$, we get:</p>
<div>
$$ 
\begin{multline} 
 D_{KL}(p(x) || q(x)) = 
\sum_x P(x) *  \\
\hspace{25mm}- \Big[\frac{k}{2}log(2\pi) - \frac{1}{2}log(|\Sigma_1|) - \frac{1}{2} \Big[(x-p_1)^{T} \Sigma^{-1}(x-p_1)\Big] \Big] \\
\hspace{-20mm} \Big[- \Big[- \frac{k}{2}log(2\pi) - \frac{1}{2}log(|\Sigma_2|) - \frac{1}{2} \Big[(x-p_2)^{T} \Sigma^{-1}(x-p_2)\Big]\Big]\Big]
\end{multline} 
$$
</div>
<p>$$ $$</p>
<div>
$$ 
\begin{multline} 
 D_{KL}(p(x) || q(x)) = 
\sum_x P(x) *  \\
\hspace{25mm} \xcancel{-\frac{k}{2}log(2\pi)}  \boxed{- \frac{1}{2}log(|\Sigma_1|)} - \frac{1}{2} \Big[(x-p_1)^{T} \Sigma^{-1}(x-p_1) \\
\xcancel{+\frac{k}{2}log(2\pi)} \boxed{+ \frac{1}{2}log(|\Sigma_2|)} + \frac{1}{2} \Big[(x-p_2)^{T} \Sigma^{-1}(x-p_2)\Big]
\end{multline} 
$$
</div>
<p>We can cancel out the parts that give 0, club the boxed item in one and remaining terms in one. Hence,</p>
<div>
$$ 
\begin{multline} 
 D_{KL}(p(x) || q(x)) = 
\sum_x P(x) *  \\
\hspace{25mm}  \Bigg[ \hspace{5mm} \boxed{+ \frac{1}{2}log(\frac{|\Sigma_2|}{|\Sigma_1|})} - \frac{1}{2} \Big[(x-p_1)^{T} \Sigma^{-1}(x-p_1) \Big] + \frac{1}{2} \Big[(x-p_2)^{T} \Sigma^{-1}(x-p_2)\Big]\Bigg]\tag{6}
\end{multline} 
$$
</div>
<p>$P(x)$ can be split for each of the terms and hence can be written as:</p>
<div>
$$ 
\begin{multline} 
 D_{KL}(p(x) || q(x)) = 
\sum_x P(x) * \Bigg[ \hspace{5mm} \frac{1}{2}log(\frac{|\Sigma_2|}{|\Sigma_1|})\Bigg] \\
\sum_x P(x) * \Bigg[- \frac{1}{2} \Big[(x-p_1)^{T} \Sigma^{-1}(x-p_1) \Big] \Bigg] + 
\sum_x P(x) * \Bigg[ \frac{1}{2} \Big[(x-p_2)^{T} \Sigma^{-1}(x-p_2)\Big]\Bigg] \tag{7}
\end{multline} 
$$
</div>
<p>Now let us solve the remaining terms one by one. We will start with</p>
<p>$$
\sum_x P(x) * \Bigg[ \frac{1}{2} \Big[(x-p_2)^{T} \Sigma_2^{-1}(x-p_2)\Big]\Bigg] \equiv E_p\Bigg[ \frac{1}{2} \Big[(x-p_2)^{T} \Sigma_2^{-1}(x-p_2)\Big]\Bigg]
$$</p>
<p>Using properties of trace and expectation (Appendix -1), we can re-write:</p>
<p>$$
E_p\Bigg[ \frac{1}{2} \Big[(x-p_2)^{T} \Sigma_2^{-1}(x-p_2)\Big]\Bigg]
$$ as</p>
<p>$$
= \frac{1}{2}E_p\Bigg[ tr\Big[(x-p_2)^{T} \Sigma_2^{-1}(x-p_2)\Big]\Bigg]
= E_p\Bigg[ tr\Big[\frac{1}{2} (x-p_2)^{T} \Sigma_2^{-1}(x-p_2)\Big]\Bigg]
= E_p\Bigg[ tr\Big[\frac{1}{2}(x-p_2)(x-p_2)^{T} \Sigma_2^{-1}\Big]\Bigg]
$$</p>
<p>Using propery __, we have</p>
<p>$$
= tr \Bigg[\hspace{3mm}\boxed{E_p \Bigg( \Bigg[(x-p_2)(x-p_2)^{T} \Bigg]} \frac{1}{2} \Sigma_2^{-1} \Bigg) \Bigg]
$$</p>
<p>The boxed element is nothing but a covaiance matrix $\Sigma_2$, therefore
$$
D_{KL}\Big(p(x) || q(x)\Big) =  tr \Bigg[ \Sigma_2 \frac{1}{2} \Sigma_2^{-1}] \Bigg] = tr \Bigg[ I_k \Bigg] \equiv k
$$</p>
<p>(Equations in Latex are so tough sometimes) To be continued &hellip;</p>

    </div>

    





<div class="article-tags">
  
  <a class="badge badge-light" href="/tag/technical/">technical</a>
  
</div>



<div class="share-box" aria-hidden="true">
  <ul class="share">
    
  </ul>
</div>












  
  





  
    
    
    
      
    
    
    
    <div class="media author-card content-widget-hr">
      
        
        <img class="avatar mr-3 avatar-circle" src="/author/srishti-yadav/avatar_hu9d788c3a7095d2e8067c7cba9a8e0ac1_2405876_270x270_fill_q90_lanczos_center.jpg" alt="Srishti Yadav">
      

      <div class="media-body">
        <h5 class="card-title"><a href="https://srishti.dev">Srishti Yadav</a></h5>
        <h6 class="card-subtitle">Research Assistant</h6>
        <p class="card-text">My research interests include applying computationally intensive machine learning algorithm to computer vision algoritmns.</p>
        <ul class="network-icon" aria-hidden="true">
  
    
    
    
      
    
    
    
    
    
    <li>
      <a href="mailto:srishtiy@sfu.ca" >
        <i class="fas fa-envelope"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://linkedin.com/in/srishti-yadav" target="_blank" rel="noopener">
        <i class="fab fa-linkedin"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://github.com/copperwiring" target="_blank" rel="noopener">
        <i class="fab fa-github"></i>
      </a>
    </li>
  
    
    
    
    
    
    
    
      
    
    <li>
      <a href="/files/cv.pdf" >
        <i class="ai ai-cv"></i>
      </a>
    </li>
  
</ul>

      </div>
    </div>
  












  
  
  <div class="article-widget content-widget-hr">
    <h3>Related</h3>
    <ul>
      
      <li><a href="/post/1111-11-11-we-have-so-much-in-common/">We Have So Much In Common: Modeling Semantic Relational Set Abstractions In Videos</a></li>
      
      <li><a href="/post/1111-11-11-presence-only-geographical-priors-for-fine-grained-image-classification/">Presence-Only Geographical Priors for Fine-Grained Image Classification</a></li>
      
      <li><a href="/post/1111-11-11-the-unreasonable-effectiveness-of-noisy-data-for-fine-grained-recognition/">The Unreasonable Effectiveness of Noisy Data for Fine-Grained Recognition</a></li>
      
      <li><a href="/post/1111-11-11-few-shot-learning-basic-concepts/">Few Shot Learning</a></li>
      
      <li><a href="/post/1111-11-11-hyperopt/">Hyperopt: A tool for parameter tuning</a></li>
      
    </ul>
  </div>
  



  </div>
</article>

      

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js" integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js" integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.js" integrity="sha256-yt2kYMy0w8AbtF89WXb2P1rfjcP/HTHLT7097U8Y5b8=" crossorigin="anonymous"></script>

      

      
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/highlight.min.js" integrity="sha256-eOgo0OtLL4cdq7RdwRUiGKLX9XsIJ7nGhWEKbohmVAQ=" crossorigin="anonymous"></script>
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/languages/r.min.js"></script>
        
      

    

    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.5.1/leaflet.js" integrity="sha256-EErZamuLefUnbMBQbsEqu1USa+btR2oIlCpBJbyD4/g=" crossorigin="anonymous"></script>
    

    
    
    <script>const code_highlighting = true;</script>
    

    
    
    <script>const isSiteThemeDark = false;</script>
    

    

    
    

    

    
    

    
    

    
    

    
    

    
    
    
    
    
    
    
    
    
    
    
    
    <script src="/js/academic.min.6f7ce8be710290b8c431bbc97f405d15.js"></script>

    






  
  
  <div class="container">
    <footer class="site-footer">
  

  <p class="powered-by">
    
  </p>

  
  






  <p class="powered-by">
    Powered by the
    <a href="https://sourcethemes.com/academic/" target="_blank" rel="noopener">Academic theme</a> for
    <a href="https://gohugo.io" target="_blank" rel="noopener">Hugo</a>.

    
    <span class="float-right" aria-hidden="true">
      <a href="#" class="back-to-top">
        <span class="button_icon">
          <i class="fas fa-chevron-up fa-2x"></i>
        </span>
      </a>
    </span>
    
  </p>

</footer>

  </div>
  

  
<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        <pre><code class="tex hljs"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

</body>
</html>
